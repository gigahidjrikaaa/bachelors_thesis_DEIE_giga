\chapter{Literature Review and Theoretical Background}
\label{chap:literature_review}

This chapter establishes the academic context for the research. It begins by surveying the existing literature on AI applications in mental health and student support to identify the limitations of current approaches. It then details the theoretical framework and enabling technologies that provide the foundation for the proposed solution. Finally, it synthesizes these areas to formally identify the research gap this thesis addresses.

% ==================================================================================
% ==================================================================================
% SECTION 1: Literature Review
% ==================================================================================
% ==================================================================================

\section{Literature Review: The Landscape of AI in University Mental Health Support}
\label{sec:literature_review}

This review surveys existing research at the intersection of artificial intelligence, institutional support systems, and student mental health. The aim is to contextualize the present work by examining the evolution and limitations of current approaches, thereby setting the stage for the introduction of a more advanced, agentic framework.

\subsection{Conversational Agents for Mental Health Support}

The application of conversational agents in mental health has evolved significantly, from early experiments in simulating dialogue to sophisticated, evidence-based therapeutic tools. This evolution reveals both the immense potential of these technologies and the persistent operational limitations that motivate the current research.

\subsubsection{Evolution from Rule-Based Systems to LLM-Powered Agents}
The concept of using a computer program for therapeutic dialogue dates back to Weizenbaum's ELIZA (1966), a system that used simple keyword matching and canned response templates to mimic a Rogerian psychotherapist \cite{shum2018conversationalsystems,alamin2024historychatbots}. While a landmark in human-computer interaction, ELIZA and subsequent rule-based systems lacked any true semantic understanding, memory, or capacity for evidence-based intervention. Their primary limitation was their inability to move beyond superficial pattern recognition, leading to brittle and often nonsensical conversations when faced with inputs outside their predefined rules \cite{shum2018conversationalsystems}.

The advent of Large Language Models (LLMs) has catalyzed a paradigm shift. Modern conversational agents, powered by Transformer architectures, can generate fluent, empathetic, and context-aware responses. These models are pre-trained on vast text corpora, enabling them to understand linguistic nuance and generate human-like text. This has allowed for the development of agents that can engage in more meaningful, multi-turn conversations, moving beyond simple question-answering to provide more substantive support \cite{alamin2024historychatbots}.

\subsubsection{Therapeutic Applications and Efficacy}
Contemporary mental health chatbots leverage LLMs to deliver a range of evidence-based interventions. A primary application is the delivery of psychoeducation and structured exercises from therapeutic modalities like Cognitive Behavioral Therapy (CBT). Systems such as Woebot have been the subject of randomized controlled trials (RCTs), which have demonstrated their efficacy in reducing symptoms of depression and anxiety among university students by delivering daily, brief, conversational CBT exercises \cite{woebotRCT2024,eltahawy2024robotsdottherapy}. Other platforms, like Tess, have shown similar positive outcomes by providing on-demand emotional support and coping strategies.

These tools offer several key advantages:
\begin{itemize}
    \item \textbf{Accessibility and Scalability:} They are available 24/7, overcoming the time and resource constraints of traditional human-led services.
    \item \textbf{Anonymity:} They provide a non-judgmental and anonymous space for users to disclose their feelings, which can lower the barrier for individuals who fear stigma \cite{kang2025chatbotstigma}.
\end{itemize}

\subsubsection{The Dominant Reactive Paradigm and Its Limitations}
Despite their technological sophistication and therapeutic potential, the fundamental operational model of modern mental health applications remains overwhelmingly \textbf{reactive}. This model, common in service design, operates on a "break-fix" basis, where service delivery is initiated only after a user—in this case, a student—self-identifies a problem and actively seeks a solution \cite{freeman2025competitionalgorithms}. They are designed as standalone tools that depend on the student to possess the self-awareness to recognize their distress, the motivation to seek help, and the knowledge of the tool's existence.

Critically, this limitation is not unique to technology; \textbf{the traditional, in-person counseling model is equally reactive}. The standard university mental health service operates on an appointment-based system where students must: (1) recognize their own distress, (2) navigate the institutional referral process, (3) schedule an appointment (often facing multi-week waitlists), and (4) attend the session during limited office hours \cite{baik2019universities, gallagher2023counselor}. 

This places the entire burden of initiation on the student, creating the same fundamental barrier across both technological and traditional systems: \textbf{it assumes students will self-identify their distress and actively seek help}. Research demonstrates that this assumption is systematically violated. Stigma, lack of mental health literacy, and a desire for self-reliance all contribute to low help-seeking rates \cite{corrigan2009stigmahelpseeking, patel2022helpseekingcollege}. More critically, the very symptoms of conditions like depression—including anhedonia, executive dysfunction, and social withdrawal—actively impair the cognitive and motivational capacities required to initiate help-seeking behavior \cite{liu2023distresshelpseeking,Alesi2024_execdysfunction}.

Therefore, both traditional and chatbot-based reactive models fail to serve the most vulnerable population: those who are in distress but do not initiate contact. A student experiencing suicidal ideation may lack the energy to schedule an appointment; a student with severe social anxiety may find the act of reaching out to be itself insurmountably distressing. This thesis proposes that a solution requires a \textbf{paradigm shift to a proactive support model} that aims to anticipate needs and intervene before a problem escalates. Drawing from principles in preventative healthcare and proactive customer relationship management, this model uses data to identify patterns and risk factors, enabling the institution to offer timely, relevant support to at-risk cohorts \cite{williams2022datadrivenhe, lyon2020datadrivenuniversity}. By continuously analyzing interaction patterns and employing automated risk detection, the proposed multi-agent framework can identify students in distress and initiate supportive contact \textit{before} they reach a crisis threshold, thereby addressing the systemic failure of all reactive support models.


\subsection{Data Analytics for Proactive Student Support}

Parallel to the development of conversational AI, the field of higher education has seen a rise in the use of data analytics to support student success. This section reviews the evolution of these analytical approaches, from established learning analytics to the more nascent field of well-being analytics, and identifies the key limitations that motivate the design of the agents.

\subsubsection{Learning Analytics for Academic Intervention}
The domain of \textbf{Learning Analytics} is well-established and focuses on the "measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs" \cite{siemens2011learninganalytics}. Typically, these systems analyze data from institutional sources such as the Learning Management System (LMS), student information systems, and library databases. By modeling variables like assignment submission times, forum participation, and grades, institutions can build predictive models to identify students at high risk of academic failure or dropout \cite{banihashem2022predictivedropout}. These systems have proven effective in enabling timely academic interventions, such as targeted tutoring or advisor outreach, thereby improving student retention and success rates.

\subsubsection{The Challenge of Well-being Analytics}
More recently, researchers have attempted to extend the principles of learning analytics to the more complex and sensitive domain of student well-being. The goal is to create early-warning systems by identifying behavioral proxies for mental distress. Studies have explored the use of non-academic data sources, such as campus card usage for building access, meal plan data, and social event attendance, to find correlations with well-being outcomes \cite{paolucci2024wellbeinganalytics}. For example, a sudden decrease in social activity or irregular campus attendance could be interpreted as a potential indicator of withdrawal or depression.

However, this approach is fraught with significant theoretical and practical challenges. Firstly, the "signal-to-noise" ratio is extremely low; the link between such indirect behavioral data and a student's internal mental state is often weak, correlational, and highly prone to misinterpretation \cite{masiello2024privacyethics}. A student may miss meals for many reasons other than depression. Secondly, these methods raise profound ethical questions regarding student privacy and surveillance, as they involve monitoring non-academic aspects of student life, often without explicit, ongoing consent for this specific purpose \cite{masiello2024privacyethics,paolucci2024wellbeinganalytics}.

A more direct, and arguably more ethical, source of data is the language students use when interacting with university services. The text from chat logs, when properly anonymized, provides a direct window into student concerns. The application of sentiment analysis and topic modeling to this textual data can yield far more reliable insights into the specific stressors affecting the student population at any given time. This approach, which is central to the design of the Analytics Agent, shifts the focus from inferring mental state from indirect behaviors to directly analyzing the expressed concerns of the student body \cite{paolucci2024wellbeinganalytics}.

\subsubsection{The Insight-to-Action Gap}
Whether based on academic, behavioral, or textual data, a critical limitation plagues nearly all current analytical systems in higher education: the \textbf{insight-to-action gap} \cite{jorno2018actionableinsight}. The output of these systems is almost universally a dashboard, a report, or an alert delivered to a human administrator (e.g., a counselor, dean, or advisor) \cite{susnjak2022dashboard}. This administrator must then manually interpret the data, decide on an appropriate intervention strategy, and execute it.

This manual process creates a severe bottleneck that fundamentally limits the scalability, speed, and personalization of any proactive effort \cite{kaliisa2023hypedashboards}. An administrator may be able to respond to a handful of individual alerts, but they cannot manually orchestrate a personalized outreach campaign to hundreds of students who may be exhibiting early signs of exam-related stress identified by a topic model. The manual-execution step prevents the institution from fully capitalizing on the proactive insights generated by its analytical systems. It is this specific gap that the proposed \textbf{agentic framework} is designed to close. By having the Aika Meta-Agent orchestrate the proactive generation of therapeutic plans (via the TCA) and automated administrative workflows (via the CMA), the system automates the link between data-driven insight and scalable, targeted action.

% ============================================================
% ============================================================
% SECTION 2: Theoretical Background
% ============================================================
% ============================================================

\section{Theoretical Background}
\label{sec:theoretical_background}

To address the limitations of reactive, disconnected support systems, a new architectural approach is required. This section details the theoretical framework and enabling technologies that provide the foundation for the proposed agentic AI system. These concepts are presented as the necessary components to build a proactive, integrated, and autonomous solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SubSection: Foundational Principles of the Framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Foundational Principles of the Framework}
\label{subsec:foundational_principles}

Beyond the technical architecture, the proposed framework is grounded in several key strategic and ethical principles that justify its design and purpose. These concepts from service design, management science, and data ethics provide the theoretical motivation for shifting how institutional support is delivered.

\subsubsection{Data-Driven Decision-Making in Higher Education}
The concept of \textbf{Data-Driven Decision-Making (DDDM)} posits that strategic decisions should be based on objective data analysis and interpretation rather than solely on intuition or tradition \cite{williams2022datadrivenhe, lyon2020datadrivenuniversity}. In higher education, this has manifested as the field of learning analytics, where student data is used to improve learning outcomes and retention. This framework extends that principle to student well-being. The \textbf{Insights Agent} is the core enabler of DDDM for the university's support services. By autonomously processing anonymized interaction data to identify trends, sentiment shifts, and emerging topics of concern, it provides administrators with actionable, empirical evidence. This allows the institution to move beyond anecdotal evidence and allocate resources, such as workshops, counselors, or targeted information campaigns, to where they are most needed, thereby optimizing the efficiency and impact of its support ecosystem \cite{popoola2025privacyawareframework}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SubSection: Agentic AI, Multi-Agent Systems, and LLMs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Agentic AI and Multi-Agent Systems (MAS)}
\label{subsec:agentic_ai}

The paradigm of Artificial Intelligence (AI) has evolved significantly from systems that perform singular, reactive tasks to those that exhibit autonomous, proactive, and social behaviors. A cornerstone of this evolution is the concept of an \textbf{intelligent agent}. An agent is not merely a program; it is a persistent computational entity with
a degree of autonomy, situated within an environment, which it can both perceive and upon which it can act to achieve a set of goals or design objectives \cite{wooldridge1995intelligentagents}. The defining characteristic of an agent is its \textbf{autonomy}, its capacity to operate independently, making decisions and initiating actions without direct, constant human intervention. This is distinct from traditional objects, which are defined by their methods and attributes but do not exhibit control over their own behavior \cite{wooldridge2009introductionmas}.

To operationalize this concept, this thesis formally introduces a framework built upon \textbf{four specialized intelligent agents} (STA, TCA, CMA, IA) that form the \textbf{Safety Agent Suite}, orchestrated by a central \textbf{Meta-Agent (Aika)}. \textbf{Critically, the Aika Meta-Agent is the sole user-facing component}---all user interactions occur exclusively through Aika's conversational interface, which internally orchestrates specialist agent invocations as needed. This design ensures a consistent user experience while enabling modular, specialized intelligence. Each specialist agent operates transparently in the background, invoked conditionally based on user role and intent. Together they form the core of the proposed proactive support system. The framework components are:
\begin{itemize}
    \item The \textbf{Aika Meta-Agent}, responsible for: (1) serving as the sole user-facing conversational interface for all stakeholders, (2) performing immediate Tier 1 risk screening via structured JSON responses from Gemini API, (3) context-aware routing to specialist agents based on user role and intent classification, (4) synthesizing specialist outputs into coherent, role-appropriate conversational responses, and (5) role-based access control enforcement.
    \item The \textbf{Safety Triage Agent (STA)}, operating in the background to perform comprehensive conversation-level risk analysis (Tier 2) at conversation end, identifying cumulative risk patterns and recommending proactive follow-up interventions.
    \item The \textbf{Therapeutic Coach Agent (TCA)}, operating entirely in the background to generate personalized, evidence-based CBT intervention plans and coping strategies that students access asynchronously via their dashboard. TCA does not participate in real-time conversations.
    \item The \textbf{Case Management Agent (CMA)}, invoked conditionally through Aika when: (1) immediate crisis escalation is required (high/critical risk detected), (2) students/staff request appointment scheduling, or (3) counselors initiate case management workflows. CMA handles clinical case workflows, counselor assignment, and SLA tracking.
    \item The \textbf{Insights Agent (IA)}, operating in the background for scheduled analytics, but invocable on-demand through Aika when administrators/counselors request analytics queries (e.g., ``show trending topics,'' ``case statistics for November''). IA performs privacy-preserving data analysis and trend identification on anonymized conversation data.
\end{itemize}

The theoretical underpinnings of these agents' architecture and behavior are drawn from established models of rational agency and multi-agent systems, as detailed below.

Fundamentally, an agent's operation is defined by a continuous cycle of perception, reasoning (or deliberation), and action. It perceives its environment through virtual \textbf{sensors} (e.g., data feeds, API calls, database queries) and influences that environment through its \textbf{actuators} (e.g., sending emails, generating reports, invoking other services) \cite{yan2024explainablebdi}. A prominent and highly relevant architecture for designing such goal- oriented agents is the \textbf{Belief-Desire-Intention (BDI)} model \cite{rao1995bdi, yan2024explainablebdi}. This model provides a framework for rational agency that mirrors human practical reasoning:

\begin{itemize}
    \item \textbf{Beliefs:} This represents the informational state of the agent, its knowledge about the environment, which may be incomplete or incorrect. For the \textbf{Insights Agent}, beliefs correspond to the current understanding of student well-being trends derived from anonymized data.
    \item \textbf{Desires:} These are the motivational states of the agent, representing the objectives or goals it is designed to achieve. Desires can be seen as the potential tasks the agent could undertake, such as the \textbf{Therapeutic Coach Agent's} overarching goal to "deliver personalized coaching."
    \item \textbf{Intentions:} This represents the agent's commitment to a specific plan or course of action. An intention is a desire that the agent has chosen to actively pursue. For instance, the \textbf{Safety Triage Agent}, upon identifying a high-severity conversation, forms an intention to immediately route the user to emergency resources.
\end{itemize}

The BDI framework allows for the design of agents that are not merely reactive but are proactive and deliberative, capable of reasoning about how to best achieve their goals given their current beliefs about the world \cite{wooldridge2009introductionmas, rao1995bdi}.

To formally ground the proposed framework in this established model, the roles and logic of each of the five framework components (four specialist agents plus the orchestrating meta-agent) are mapped to the BDI components in Table \ref{tab:bdi_mapping}. This mapping clarifies how each component perceives its environment, formulates its objectives, and decides on a concrete course of action, allowing for the design of agents that are not merely reactive but are proactive and deliberative, capable of reasoning about how to best achieve their goals given their current beliefs about the world.

% This table maps the proposed intelligent agents to the Belief-Desire-Intention (BDI) model of rational agency.
\begin{table}[htbp]
    \centering
    \caption{Mapping of the Agentic Framework to the BDI Model.}
    \label{tab:bdi_mapping}
    \footnotesize % Smaller font for better fit
    \renewcommand{\arraystretch}{1.3} % Adjusting row spacing
    \setlength{\tabcolsep}{4pt} % Reduce column separation
    \begin{tabular}{p{0.13\textwidth}p{0.27\textwidth}p{0.25\textwidth}p{0.27\textwidth}}
        \toprule
        \textbf{Agent} & 
        \textbf{Beliefs} \newline \textit{(Informational State)} & 
        \textbf{Desires} \newline \textit{(Motivational Goals)} & 
        \textbf{Intentions} \newline \textit{(Committed Plans)} \\
        \midrule

        \textbf{STA} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item User's conversation history
            \item Severity classification model
            \item Emergency resources directory
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Assess immediate risk level
            \item Provide appropriate support
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Escalate high-severity cases
            \item Display emergency contacts
        \end{itemize} \\
        \midrule

        \textbf{TCA} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item User goals \& history
            \item Evidence-based intervention library (CBT)
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Deliver personalized coaching
            \item Guide through exercises
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Deliver specific CBT exercise
            \item Provide empathetic responses
        \end{itemize} \\
        \midrule

        \textbf{CMA} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Clinical case status
            \item Counselor availability
            \item User appointment requests
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Manage case workflows
            \item Schedule appointments
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Find available appointment slots
            \item Create and update case notes
        \end{itemize} \\
        \midrule
        
        \textbf{IA} & 
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Anonymized conversation database
            \item Last report timestamp
            \item Known topic models
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Identify emerging trends
            \item Quantify sentiment shifts
        \end{itemize} &
        \begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
            \item Generate weekly summary reports
            \item Execute database queries
        \end{itemize} \\
        \midrule
        
        \textbf{Aika Meta-Agent} &
        \begin{itemize} \itemsep0em
            \item User role and authentication context (student/counselor/admin).
            \item Conversation history and session state across all agents.
            \item Routing policies and agent capability mappings.
            \item Current risk assessment from STA (if applicable).
        \end{itemize} &
        \begin{itemize} \itemsep0em
            \item To provide a unified, role-appropriate interface for all users.
            \item To ensure safety-first routing for all student interactions.
            \item To coordinate multi-agent workflows seamlessly.
        \end{itemize} &
        \begin{itemize} \itemsep0em
            \item Upon receiving a user message, form an intention to classify intent and route to appropriate specialist(s).
            \item To synthesize specialist responses with role-consistent personality.
            \item To maintain conversational coherence across agent transitions.
        \end{itemize} \\
        \bottomrule
    \end{tabular}
\end{table}

When multiple agents, each with its own goals and capabilities, co-exist and interact within a shared environment, they form a \textbf{Multi-Agent System (MAS)}. An MAS is a system in which the overall intelligent behavior and functionality are a product of the collective, emergent dynamics of its constituent agents \cite{burguillo2017multiagentsystems, petrova2025agenticweb}. The power of an MAS lies in its ability to solve problems that would be difficult or impossible for a monolithic system or a single agent to handle. This is achieved through social interaction, primarily:

\begin{itemize}
    \item \textbf{Coordination and Cooperation:} Agents must coordinate their actions to avoid interference and cooperate to achieve common goals. In this thesis, the \textbf{Insights}, \textbf{Therapeutic Coach}, \textbf{Safety Triage}, and \textbf{Case Management} agents must cooperate: the Insights Agent provides the data-driven insights (beliefs) that the Therapeutic Coach Agent uses to form its outreach plans (intentions), while the Safety Triage Agent handles immediate, real-time needs that may fall outside the other agents' scopes, and the Case Management Agent manages the administrative follow-up.
    \item \textbf{Negotiation:} When agents have conflicting goals or must compete for limited resources, they must be able to negotiate to find a mutually acceptable compromise \cite{paurobally2002rationalagents, agerri2006unifiedcommunication}.
    \item \textbf{Communication:} Effective interaction requires a shared Agent Communication Language (ACL), such as FIPA-ACL or KQML, which defines the syntax and semantics for messages, allowing agents to perform actions like requesting information, making proposals, and accepting or rejecting tasks \cite{fornara2003interaction, williams2025multicommunication}.
\end{itemize}

Therefore, this thesis leverages the MAS paradigm by designing a framework composed of four specialized, collaborative agents coordinated by a meta-agent orchestrator. Their individual, goal-directed behaviors, orchestrated within a hierarchical architecture, work in concert to achieve the overarching systemic objective: transforming institutional mental health support from a reactive model to a proactive, data-driven ecosystem.

\subsubsection{Formal Logic of Agent Orchestration}
\label{subsubsec:agent_decision_functions}

To operationalize the BDI model, the decision-making logic of the Safety Agent Suite is formalized as a set of mapping functions. This formalization clarifies how the Aika Meta-Agent orchestrates the specialized agents based on user inputs and context.

\paragraph{Aika Meta-Agent (Orchestrator)}
Think of the Aika Meta-Agent as the "front desk receptionist" of the system. Its job is to handle the immediate interaction. When a user sends a message ($m_t$), Aika considers who the user is ($\text{UserRole}$) and performs two simultaneous tasks: it classifies what the user wants ($I$) and checks for any immediate danger ($R_{immediate}$). This initial triage is defined in Equation \ref{eq:aika_func}:

\begin{equation}
\label{eq:aika_func}
(I, R_{immediate}) = f_{Aika}(m_t, \text{UserRole})
\end{equation}

In Equation \ref{eq:aika_func}, $f_{Aika}$ represents the meta-agent's cognitive processing (powered by the LLM). It takes the raw input message and the user's role as variables and outputs a tuple containing the Intent ($I$) and the Immediate Risk Level ($R_{immediate}$).

Based on this initial assessment, Aika must decide where to route the conversation. This is analogous to the receptionist deciding whether to call security, schedule an appointment, or just answer a simple question. This routing logic is formalized in Equation \ref{eq:aika_action}:

\begin{equation}
\label{eq:aika_action}
\text{Action} = \begin{cases}
\text{Escalate to CMA} & \text{if } R_{immediate} \in \{\text{High}, \text{Critical}\} \\
\text{Invoke Tools} & \text{if } I \in \{\text{Scheduling}, \text{Info}\} \\
\text{Direct Response} & \text{otherwise}
\end{cases}
\end{equation}

Equation \ref{eq:aika_action} describes a piecewise function where the output $\text{Action}$ depends on the values of $R_{immediate}$ and $I$. If the risk is high, the system escalates; if the intent requires a specific tool, it invokes it; otherwise, it handles the query directly.

\paragraph{Safety Triage Agent (STA)}
While Aika handles the "now," the Safety Triage Agent acts as a background investigator. It doesn't just look at the last message; it reviews the entire conversation history ($H_t$) to find patterns that might indicate a deeper problem, such as slowly increasing anxiety. This deep-dive analysis produces a cumulative risk score ($R_{cumulative}$), as defined in Equation \ref{eq:sta_func}:

\begin{equation}
\label{eq:sta_func}
R_{cumulative} = f_{STA}(H_t)
\end{equation}

In Equation \ref{eq:sta_func}, the variable $H_t$ represents the full transcript of the session up to time $t$. The function $f_{STA}$ processes this large context window to output the comprehensive risk assessment.

\paragraph{Therapeutic Coach Agent (TCA)}
If the system identifies a need for support that isn't an emergency (e.g., moderate stress), the Therapeutic Coach Agent steps in. Think of the TCA as a counselor who prepares a "take-home" care plan. It uses the conversation history ($H_t$) and the student's profile ($\text{UserProfile}$) to generate a personalized intervention plan ($P_{intervention}$), such as a set of CBT exercises. This is modeled in Equation \ref{eq:tca_func}:

\begin{equation}
\label{eq:tca_func}
P_{intervention} = f_{TCA}(H_t, \text{UserProfile})
\end{equation}

Equation \ref{eq:tca_func} shows that the intervention plan $P_{intervention}$ is a function of both what happened in the chat ($H_t$) and who the student is ($\text{UserProfile}$), ensuring the advice is tailored to the individual.

\paragraph{Case Management Agent (CMA)}
The Case Management Agent is the system's administrator. Its role is to execute concrete logistical actions ($\alpha$), such as creating a ticket in the database or booking a slot. To do this, it needs to know the current state of the case ($C_t$) and what resources are available ($Res$), like open calendar slots. This is defined in Equation \ref{eq:cma_func}:

\begin{equation}
\label{eq:cma_func}
\alpha = f_{CMA}(C_t, Res)
\end{equation}

In Equation \ref{eq:cma_func}, the output $\alpha$ represents the administrative action taken. The function $f_{CMA}$ ensures that this action is valid given the current constraints ($Res$) and case status ($C_t$).

\paragraph{Insights Agent (IA)}
Finally, the Insights Agent acts as a privacy-conscious data analyst. It looks at the data from the entire student population ($\mathcal{D}$) to answer specific questions ($\text{Query}$), producing population-level metrics ($\mu$). However, it operates under a strict constraint: it must not reveal any individual's identity. This is mathematically represented by the condition $\text{Privacy}(\mu) \ge k$, which refers to k-anonymity. This is formalized in Equation \ref{eq:ia_func}:

\begin{equation}
\label{eq:ia_func}
\mu = f_{IA}(\mathcal{D}, \text{Query}) \quad \text{s.t. } \text{Privacy}(\mu) \ge k
\end{equation}

Equation \ref{eq:ia_func} states that the metrics $\mu$ are derived from the dataset $\mathcal{D}$ and the $\text{Query}$, subject to the constraint that the privacy score of the result must meet or exceed the threshold $k$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SubSection: Explainable AI (XAI) and Trust in Automation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Explainable AI (XAI) and Trust in Automation}
\label{subsec:xai_trust}

In safety-critical domains such as mental health support, the "black box" nature of deep learning models poses a significant challenge to adoption and safety. \textbf{Explainable AI (XAI)} refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by human experts \cite{arrieta2020explainable}. 

\subsubsection{Trust in Automation}
Trust is a foundational element in the relationship between humans and automated systems. Lee and See define trust in automation as "the attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability" \cite{lee2004trust}. In the context of the Safety Agent Suite, trust must be established not only with the student users but also with the clinical administrators who oversee the system. Over-trust can lead to complacency (missing critical failures), while under-trust can lead to disuse (ignoring valid alerts).

\subsubsection{Algorithmic Transparency and Risk Reasoning}
To mitigate these risks and foster appropriate trust, the system incorporates mechanisms for \textbf{algorithmic transparency}. Specifically, the Safety Triage Agent (STA) is designed to provide not just a risk classification, but also a structured \textbf{risk reasoning} output. This aligns with the principles of "post-hoc explainability," where the model articulates the specific linguistic cues or patterns that led to a high-risk assessment (e.g., "User expressed direct intent of self-harm in previous turn"). This transparency allows human supervisors to validate the agent's decisions, ensuring that the "human-in-the-loop" can effectively audit the system's performance and intervene when necessary \cite{holzinger2019causability}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SubSection: Large Language Models (LLMs)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Large Language Models (LLMs)}
\label{subsec:llms}

Large Language Models (LLMs) are a class of deep learning models that have demonstrated remarkable capabilities in understanding and generating human-like text. The architectural foundation for virtually all modern LLMs, including the Gemini models used in this research, is the \textbf{Transformer architecture} (see Figure \ref{fig:transformer_architecture}), first introduced by Vaswani et al. \cite{vaswani2017attention}. The Transformer's key innovation is the \textbf{self-attention mechanism}, which allows the model to dynamically weigh the importance of different words in an input sequence when processing and generating language. This enables the model to capture complex, long-range dependencies and contextual relationships far more effectively than its predecessors, such as Recurrent Neural Networks (RNNs) \cite{liu2023transformersurvey, vaswani2017comparisonrnn}.

\paragraph{The Self-Attention Mechanism}
The self-attention mechanism allows the model to dynamically weigh the importance of different words in an input sequence when processing and generating language. This enables the model to capture complex, long-range dependencies and contextual relationships far more effectively than its predecessors. Modern Transformers employ \textbf{multi-head attention}, which applies multiple attention operations in parallel, enabling the model to capture diverse linguistic patterns and semantic relationships concurrently.

Mathematically, the scaled dot-product attention is defined in Equation \ref{eq:attention}:

\begin{equation}
\label{eq:attention}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

To understand Equation \ref{eq:attention}, imagine you are trying to understand the meaning of a specific word in a sentence (the Query, $Q$). To do this, you look at all the other words in the sentence (the Keys, $K$) to see which ones are relevant. The dot product $QK^T$ calculates a "relevance score" between your word and every other word. We divide by $\sqrt{d_k}$ to keep the numbers stable. The $\text{softmax}$ function then converts these scores into probabilities (weights) that sum to 1. Finally, we multiply these weights by the actual content of the words (the Values, $V$). The result is a new representation of your word that is a weighted mixture of all the relevant context around it.

The core operation of a Transformer-based model involves processing input text through a series of encoding and/or decoding layers. The process can be conceptualized as follows:
\begin{enumerate}
    \item \textbf{Tokenization and Embedding:} Input text is first broken down into smaller units called tokens. Each token is then mapped to a high-dimensional vector, or an "embedding," that represents its semantic meaning.
    \item \textbf{Positional Encoding:} Since the self-attention mechanism does not inherently process sequential order, a positional encoding vector is added to each token embedding to provide the model with information about the word's position in the sequence.
    \item \textbf{Self-Attention Layers:} The sequence of embeddings passes through multiple self-attention layers. In each layer, the model calculates attention scores for every token relative to all other tokens in the sequence, effectively learning which parts of the input are most relevant for understanding the context of each specific token.
    \item \textbf{Feed-Forward Networks:} Each attention layer is followed by a feed-forward neural network that applies further transformations to each token's representation.
    \item \textbf{Output Generation:} The model's final output is a probability distribution over its entire vocabulary for the next token in the sequence. The model then typically selects the most likely token (or samples from the distribution) and appends it to the input, repeating the process autoregressively to generate coherent text \cite{liu2023transformersurvey}.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
    node distance=0.8cm,
    block/.style={rectangle, draw=blue!60, fill=blue!10, thick, minimum width=5cm, minimum height=0.8cm, align=center, rounded corners=2pt},
    blockwide/.style={rectangle, draw=blue!60, fill=blue!10, thick, minimum width=5cm, minimum height=1.2cm, align=center, rounded corners=2pt},
    attention/.style={rectangle, draw=orange!70, fill=orange!15, thick, minimum width=4.5cm, minimum height=0.7cm, align=center, rounded corners=2pt},
    ffn/.style={rectangle, draw=green!60, fill=green!10, thick, minimum width=4.5cm, minimum height=0.7cm, align=center, rounded corners=2pt},
    output/.style={rectangle, draw=purple!60, fill=purple!10, thick, minimum width=5cm, minimum height=0.8cm, align=center, rounded corners=2pt},
    arrow/.style={->, >=stealth, thick, draw=gray!70},
    dashedarrow/.style={->, >=stealth, thick, draw=gray!70, dashed},
    label/.style={font=\small}
    ]
    
    % Input Layer
    \node[block] (input) {\textbf{Input Text}};
    
    % Tokenization
    \node[block, below=of input] (tokenizer) {\textbf{Tokenizer} \\ (Byte-Pair Encoding)};
    
    % Embedding + Positional Encoding
    \node[blockwide, below=of tokenizer] (embedding) {\textbf{Token Embedding} \\ + \\ \textbf{Positional Encoding}};
    
    % Decoder Block N (first shown)
    \node[block, below=1.2cm of embedding, fill=gray!5] (decoder_n) {\textbf{Decoder Block N}};
    
    % Multi-Head Self-Attention
    \node[attention, below=0.4cm of decoder_n] (attention_n) {\footnotesize Multi-Head Self-Attention};
    
    % Add & Norm 1
    \node[block, below=0.3cm of attention_n, minimum height=0.5cm, fill=blue!5] (addnorm1_n) {\footnotesize Add \& Norm};
    
    % Feed-Forward Network
    \node[ffn, below=0.3cm of addnorm1_n] (ffn_n) {\footnotesize Feed-Forward Network};
    
    % Add & Norm 2
    \node[block, below=0.3cm of ffn_n, minimum height=0.5cm, fill=blue!5] (addnorm2_n) {\footnotesize Add \& Norm};
    
    % Ellipsis for stacked blocks
    \node[below=0.6cm of addnorm2_n, font=\Large] (ellipsis) {$\vdots$};
    \node[right=0.1cm of ellipsis, font=\small, text=gray] {(Repeat for N layers)};
    
    % Decoder Block 1 (last shown)
    \node[block, below=0.6cm of ellipsis, fill=gray!5] (decoder_1) {\textbf{Decoder Block 1}};
    
    % Output Layers
    \node[output, below=1.0cm of decoder_1] (linear) {\textbf{Linear Layer}};
    \node[output, below=of linear] (softmax) {\textbf{Softmax}};
    \node[block, below=of softmax] (output_token) {\textbf{Next Token Probability}};
    
    % Arrows - Main flow
    \draw[arrow] (input) -- (tokenizer);
    \draw[arrow] (tokenizer) -- (embedding);
    \draw[arrow] (embedding) -- (decoder_n);
    \draw[arrow] (decoder_n) -- (attention_n);
    \draw[arrow] (attention_n) -- (addnorm1_n);
    \draw[arrow] (addnorm1_n) -- (ffn_n);
    \draw[arrow] (ffn_n) -- (addnorm2_n);
    \draw[arrow] (addnorm2_n) -- (ellipsis);
    \draw[arrow] (ellipsis) -- (decoder_1);
    \draw[arrow] (decoder_1) -- (linear);
    \draw[arrow] (linear) -- (softmax);
    \draw[arrow] (softmax) -- (output_token);
    
    % Residual connections (dashed)
    \draw[dashedarrow] (decoder_n.west) to[out=180, in=180] (addnorm1_n.west);
    \draw[dashedarrow] (addnorm1_n.west) to[out=180, in=180] (addnorm2_n.west);
    
    % Side annotation for decoder block
    \node[right=1.5cm of attention_n, text width=2.5cm, font=\footnotesize, align=left] (annotation) {
      \textbf{Decoder Block:}\\
      • Masked Self-Attention\\
      • Residual Connections\\
      • Layer Normalization
    };
    
    \draw[->, >=stealth, thick, draw=gray!50, dashed] (attention_n.east) -- (annotation.west);
    
  \end{tikzpicture}
  \caption{A simplified view of the decoder-only Transformer architecture used in generative LLMs. The model processes input embeddings through multiple layers (blocks) of masked multi-head self-attention and feed-forward networks with residual connections to predict the next token in a sequence.}
  \label{fig:transformer_architecture}
\end{figure}

This research utilizes a cloud-based API model strategy, leveraging the Gemini 2.5 family of models to balance performance, privacy, and capability. The Gemini models represent Google's state-of-the-art, natively multimodal foundation models, available in various sizes (e.g., Gemini Pro). Unlike models trained solely on text, Gemini was pre-trained from the ground up on multiple data modalities, giving it more sophisticated reasoning capabilities \cite{google2025gemini2_5}. In this framework, a powerful model like Gemini 2.5 Pro is accessed via a secure API for all agentic tasks \cite{gemini_api_docs}, from the real-time conversation handling of the Safety Triage Agent to the complex, non-sensitive tasks, such as the weekly trend analysis performed by the Insights Agent.

\subsubsection{Cloud-Based API Models: The Gemini 2.5 Family}

The framework integrates a state-of-the-art, proprietary model accessed via a cloud API. The Gemini family, specifically the flagship \textbf{Gemini 2.5 Flash} model, serves this role, providing a level of reasoning and multimodal understanding that is critical for handling the most complex tasks and ensuring system robustness. While a detailed architectural schematic is not public, in line with the proprietary nature of frontier AI models, its capabilities have been extensively documented by Google through official developer guides and announcements \cite{google2025gemini2_5,gemini_api_docs}.

Gemini 2.5 builds upon the efficient \textbf{Mixture-of-Experts (MoE) Transformer} architecture of its predecessors. In an MoE architecture, the model is composed of numerous smaller "expert" neural networks. For any given input, a routing mechanism activates only a sparse subset of these experts. This allows the model to have a very large total parameter count, enabling vast knowledge and capability, while keeping the computational cost for any single inference relatively low \cite{google2025gemini2_5}.

The strategic role of Gemini 2.5 in this framework is defined by its next-generation capabilities:
\begin{itemize}
    \item \textbf{Native Multimodality with Expressive Audio:} A significant architectural leap in Gemini 2.5 is its native handling of audio \cite{googleblogaudio2025}. Unlike models that first transcribe audio to text, Gemini 2.5 processes audio streams directly. This allows it to understand not just the words, but also the nuances of human speech such as tone, pitch, and prosody, which is invaluable for a mental health application where user sentiment is key.
    \item \textbf{Advanced Agentic Capabilities and Tool Use:} The model is explicitly designed to power advanced agents. It features more reliable and sophisticated function calling, enabling seamless integration with external tools and APIs \cite{google2025gemini2_5}. This is essential for the Case Management Agent to execute multi-step plans, such as scheduling an appointment based on a user's request.
    \item \textbf{High-Fidelity Reasoning:} As a frontier model, Gemini 2.5 serves as the high-capability engine for all requests, ensuring service continuity and the highest quality output.
\end{itemize}

By integrating Gemini 2.5 via its API, the agentic framework gains access to state-of-the-art reasoning power on demand, ensuring that it can handle a wide spectrum of tasks with both efficiency and exceptional quality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SubSection: LLM Orchestration Frameworks (LangChain)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLM Orchestration Frameworks}
\label{subsec:llm_orchestration}

While LLMs provide powerful reasoning capabilities, they are inherently stateless and lack direct access to external data or tools. An LLM, in isolation, cannot query a database, call an API, or access a private document. To build sophisticated, stateful applications that overcome these limitations, an orchestration framework is required.

\subsubsection{LangChain: The Building Blocks of LLM Applications}
\label{subsubsec:langchain_building_blocks}

\textbf{LangChain} is an open-source framework designed specifically for this purpose, providing the essential "glue" to connect LLMs with external resources and compose them into complex applications \cite{barua2024llmagentsreview,yu2025agentworkflow}. The core philosophy of LangChain is to provide modular components that can be "chained" together to create complex workflows. The most recent and fundamental abstraction in LangChain is the \textbf{LangChain Expression Language (LCEL)}. LCEL provides a declarative, composable syntax for building chains, where the pipe (`|`) operator streams the output of one component into the input of the next. Every component in an LCEL chain is a "Runnable," a standardized interface that supports synchronous, asynchronous, batch, and streaming invocations, making it highly versatile for production environments \cite{yu2025agentworkflow, pospech2025metagraph}.

A simple LCEL chain can be represented as shown in Equation \ref{eq:lcel_chain}:
\begin{equation}
\label{eq:lcel_chain}
\text{Chain} = \text{PromptTemplate} \ | \ \text{LLM} \ | \ \text{OutputParser}
\end{equation}
In this sequence, user input is first formatted by a `PromptTemplate`, the result is passed to the `LLM` for processing, and the LLM's raw output is then transformed into a structured format (e.g., JSON) by an `OutputParser`.

For this thesis, the most critical application of LangChain is its ability to create \textbf{agents}. A LangChain agent uses an LLM not just for text processing, but as a reasoning engine to make decisions. This is often based on a framework known as \textbf{ReAct (Reasoning and Acting)}, which enables the LLM to synergize reasoning and action \cite{yao2022react, barua2024llmagentsreview}. The agent is given access to a set of \textbf{Tools}, which are simply functions that can interact with the outside world (e.g., a database query function, a file reader, a web search API). The agent's operational loop, managed by an \textbf{Agent Executor}, can be formalized as an iterative process.

Let $G$ be the initial goal and $H_t$ be the history of actions and observations up to step $t$. The process at each step $t$ is:
\begin{enumerate}
    \item \textbf{Reasoning (Thought Generation):} The agent generates a thought $th_t$ and a subsequent action $a_t$ by sampling from the LLM's conditional probability distribution, given the goal and the history so far, as formalized in Equation \ref{eq:react_thought}:
    \begin{equation}
    \label{eq:react_thought}
    (th_t, a_t) \sim p(th, a | G, H_{t-1}; \theta_{LLM})
    \end{equation}
    The prompt to the LLM contains the goal and the trajectory of previous thoughts, actions, and observations, guiding its next decision.

    \item \textbf{Action Execution:} The Agent Executor parses $a_t$ to identify the chosen tool and its input, then executes it to produce an observation, $o_t$ (Equation \ref{eq:react_action}).
    \begin{equation}
    \label{eq:react_action}
    o_t = \text{ExecuteTool}(a_t)
    \end{equation}

    \item \textbf{History Augmentation:} The new observation is appended to the history, forming the context for the next iteration, as shown in Equation \ref{eq:react_history}.
    \begin{equation}
    \label{eq:react_history}
    H_t = H_{t-1} \oplus (a_t, o_t)
    \end{equation}
    This loop continues until the LLM determines the goal $G$ is met and generates a final answer.
\end{enumerate}
This iterative loop is what transforms a passive LLM into a proactive, problem-solving agent. For example, the \textbf{Insights Agent} in this framework, when tasked with "summarizing student stress trends," would use this loop to formulate a SQL query (Thought and Action), execute it (Observation), and then use the results to generate a final summary. This orchestration is fundamental to enabling the autonomous capabilities central to this thesis.

% --- DESCRIPTION FOR A DIAGRAM ---
% You could add a diagram here with the following description:
% \begin{figure}[htbp]
%   \centering
%   % \includegraphics[width=\textwidth]{path/to/your/diagram.png}
%   \caption{Conceptual Diagram of a LangChain Agent based on the ReAct Framework. The diagram shows a central box labeled "Agent (LLM Reasoning Core)". An arrow labeled "Goal/Input" points to this box. From the agent box, several arrows point outwards to a set of boxes labeled "Tools", such as "Database Query", "Web Search", and "Calculator". The agent box has a looping arrow on it labeled "Thought -> Action -> Observation". An arrow labeled "Final Answer" points away from the agent box, showing the result after the loop is complete. This visualizes how the LLM core decides which tool to use to accomplish a goal.}
%   \label{fig:langchain_agent}
% \end{figure}

\subsubsection{LangGraph: Orchestrating Multi-Agent Systems}

While LangChain's standard agent executors are powerful, they are often designed for linear, sequential execution paths. For a sophisticated multi-agent system like the \textbf{Safety Agent Suite}, where agents must collaborate, hand off tasks, and operate in a cyclical, stateful manner, a more robust orchestration mechanism is required. This is the role of \textbf{LangGraph}, an extension of LangChain designed for building durable, stateful, multi-agent applications by modeling them as cyclical graphs \cite{yang2025aiagentprotocols,rauch2025modularagents}.

The core concept of LangGraph is to represent the agentic workflow as a \textbf{state graph}. This is a directed graph where nodes represent functions or LLM calls (the "work" to be done) and edges represent the conditional logic that directs the flow of execution from one node to another. A central \textbf{State} object is passed between nodes, allowing each agent or tool to read the current state, perform its function, and then update the state with its results. This creates a persistent, auditable record of the agent's operations \cite{mathew2025largelanguagemodelagents,pospech2025metagraph}.

A LangGraph workflow can be defined by the following components:
\begin{itemize}
    \item \textbf{State Graph:} The overall structure, $G = (N, E)$, where $N$ is a set of nodes and $E$ is a set of directed edges. The graph's state is explicitly defined by a state object that is passed and updated throughout the execution.
    \item \textbf{Nodes:} Each node represents an agent or a tool. When called, a node receives the current state object as input and returns a dictionary of updates to be applied to the state. For example, the `Safety Triage Agent` node would take the user's message from the state, process it, and return an update specifying the assessed risk level.
    \item \textbf{Edges:} Edges connect the nodes and control the flow of the application. LangGraph supports \textbf{conditional edges}, which are crucial for agentic behavior. After a node executes, a routing function is called to inspect the current state and decide which node to move to next \cite{yu2025agentworkflow,pospech2025metagraph}. For example, after the `Safety Triage Agent` runs, a conditional edge might route the workflow to the `Therapeutic Coach Agent` if the risk is moderate, or to the `Case Management Agent` if the risk is critical.
\end{itemize}

\paragraph{State Transition Semantics}
The stateful execution of a LangGraph workflow is governed by formal state update rules. Each node in the graph transforms the shared state through a state update function, defined in Equation \ref{eq:state_update}:

\begin{equation}
\label{eq:state_update}
S_{t+1} = \text{node}_i(S_t) = S_t \oplus \Delta S_i
\end{equation}

Equation \ref{eq:state_update} describes how the conversation's context evolves. Think of $S_t$ as a shared project notebook at time $t$. When an agent (node $i$) does some work, it produces a result, $\Delta S_i$ (e.g., a new risk score). It doesn't throw away the notebook; instead, it uses the merging operator $\oplus$ to add its new note to the existing pages. The result, $S_{t+1}$, is the updated notebook containing everything from before plus the new information, ready for the next agent.

Conditional edges implement routing logic via predicate functions that inspect this shared state. For the Safety Agent Suite, the routing after risk assessment is formalized to include a \textbf{confidence threshold} ($\tau$), ensuring that uncertain predictions are automatically escalated for human review. The routing function is defined in Equation \ref{eq:routing_logic}:

\begin{equation}
\label{eq:routing_logic}
\text{next}(S_t) = \begin{cases}
\text{escalate\_to\_cma} & \text{if } S_t.\text{risk\_level} \geq 2 \lor \text{conf}(S_t.\text{risk\_level}) < \tau \\
\text{provide\_coaching} & \text{if } S_t.\text{risk\_level} = 1 \land \text{conf}(S_t.\text{risk\_level}) \geq \tau \\
\text{END} & \text{if } S_t.\text{risk\_level} = 0 \land \text{conf}(S_t.\text{risk\_level}) \geq \tau
\end{cases}
\end{equation}

Equation \ref{eq:routing_logic} acts like a traffic controller at a junction. It looks at the current state of the notebook ($S_t$). Specifically, it checks the risk level and how confident the agent is in that assessment ($\text{conf}$).
\begin{itemize}
    \item If the risk is high ($\ge 2$) OR the agent is unsure (confidence $< \tau$), the traffic is directed to the Case Manager for human review.
    \item If the risk is moderate ($= 1$) AND the agent is sure, it goes to the Coach.
    \item If the risk is low ($= 0$) AND the agent is sure, the interaction ends.
\end{itemize}
This logic ensures that high-stakes or uncertain situations are always escalated, while routine cases are handled automatically.

This cyclical, stateful approach provides several key advantages for this framework:
\begin{enumerate}
    \item \textbf{Explicit Multi-Agent Collaboration:} LangGraph allows for the explicit definition of workflows where different agents are called in sequence or in parallel, and their outputs are used to inform the next step \cite{tran2025multiagentcollaboration, mathew2025largelanguagemodelagents}. This is essential for the \textbf{Safety Agent Suite}, where the `Insights Agent`'s output must trigger the `Therapeutic Coach Agent`.
    \item \textbf{State Management and Durability:} Because the state is explicitly managed, the agent's "memory" of the conversation and its previous actions is robust. The graph's execution can be paused, resumed, and inspected, which is vital for long-running, interactive coaching sessions.
    \item \textbf{Flexibility and Control:} Unlike the more constrained loops of standard agent executors, LangGraph allows for the creation of arbitrary cycles. An agent can loop, retry a tool call if it fails, or route to a human-in-the-loop for verification, providing a much higher degree of control and reliability for a safety-critical application \cite{tang2025autoagent, aquinodeazevedo2025ragtomultiagent}.
\end{enumerate}

By using LangGraph to orchestrate the \textbf{Safety Agent Suite}, this framework moves beyond simple, linear agentic loops and implements a true multi-agent system capable of complex, stateful, and collaborative problem-solving \cite{yang2025aiagentprotocols, tran2025multiagentcollaboration}.


% ============================================================
% ============================================================
% Section: Synthesis and Identification of the Research Gap
% ============================================================
% ============================================================

\section{Synthesis and Identification of the Research Gap}

The preceding review of the literature and theoretical landscape reveals a critical disconnect. On one hand, the field has produced increasingly sophisticated but fundamentally \textbf{reactive} conversational agents for mental health. On the other, it has developed proactive institutional analytics that remain bottlenecked by a reliance on \textbf{manual intervention}. The failure of the existing literature is not in the individual components, but in the lack of integration between them.

This creates a significant and unaddressed research gap: the need for an \textbf{integrated, autonomous, and proactive framework} that can systemically bridge the chasm from data-driven insight to automated, personalized intervention and administrative action. Current systems are not designed as a cohesive ecosystem. The analytical tools do not automatically trigger the intervention tools, the conversational agents do not seamlessly hand off tasks to administrative agents, and the user-facing support does not operate with an awareness of the broader institutional context provided by analytics.

The central argument of this thesis is that the next frontier in institutional mental health support lies not in the incremental improvement of any single component, but in the \textbf{synergistic integration of multiple specialized agents} into a single, closed-loop system. Such a system, architected as a Multi-Agent System (MAS), is capable of emergent behaviors that are more than the sum of its parts.

Therefore, this research directly addresses the identified gap by proposing and prototyping a novel agentic AI framework, the \textbf{Safety Agent Suite}, where:
\begin{itemize}
    \item An \textbf{Insights Agent (IA)} autonomously identifies trends, moving beyond the static dashboards of current well-being analytics and creating actionable intelligence.
    \item A \textbf{Therapeutic Coach Agent (TCA)} and a \textbf{Safety Triage Agent (STA)} act on this intelligence and on real-time user needs, providing both proactive, personalized coaching and immediate, context-aware crisis support. They function as the intelligent front-door to the support ecosystem, overcoming the limitations of purely reactive chatbots.
    \item A \textbf{Case Management Agent (CMA)} closes the "insight-to-action" loop on an administrative level, automating the workflows for clinical case management and resource allocation that currently render proactive models inefficient and unscalable.
\end{itemize}

By designing and evaluating a system where these agents work in concert, orchestrated by LangGraph, this thesis pioneers a holistic solution that is fundamentally more proactive, scalable, and efficient than the disparate tools described in the current literature.
