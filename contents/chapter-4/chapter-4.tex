\chapter{Implementation and Evaluation}
\label{chap:evaluation}

This chapter reports how the prototype was exercised and what we learned from it. The focus is on the agents and their behavior in safety‑relevant scenarios. We keep the scope practical and transparent so results can be reproduced and audited.

\input{contents/chapter-4/implementation_artifact.tex}
\input{contents/chapter-4/monitoring_section.tex}

\section{Evaluation Scope and Methodology}
\label{sec:evaluation_scope}

\subsection{Scope Boundaries and Rationale}

This evaluation adopts a \textbf{proof-of-concept validation approach} appropriate for bachelor's-level Design Science Research. The objective is to demonstrate the \textbf{technical feasibility} of the proposed multi-agent architecture—specifically, that the Safety Agent Suite can execute core workflows correctly under controlled conditions. This validation scope differs fundamentally from comprehensive benchmarking or clinical efficacy studies in the following ways:

\begin{itemize}
    \item \textbf{Sample Sizes:} Modest test set sizes (50 crisis conversation scenarios, 10 orchestration flows, 10 coaching scenarios, code review for privacy) enable focused validation of architectural correctness without requiring extensive data collection infrastructure. This is consistent with DSR artifact evaluation conventions \cite{hevner2004design}, where initial validation focuses on demonstrating capability rather than exhaustive performance characterization.
    \item \textbf{Simulation-Based Evaluation (In-Silico):} Given the sensitive nature of mental health interventions, this study adopts a simulation-based evaluation strategy. Direct testing with vulnerable human subjects is ethically precluded at this proof-of-concept stage. Therefore, synthetic datasets were generated to rigorously stress-test the safety protocols without risking patient harm \cite{Kamarzarin2025Simulation}.
    \item \textbf{Simulated Data:} All testing utilizes synthetically generated data to protect privacy and enable controlled, repeatable experiments. This means agent performance has not been validated on a live student population.
    \item \textbf{Single-Rater Assessment with LLM Validation:} Response quality is primarily assessed by the researcher using a structured rubric based on clinical guidelines \cite{american2013guidelines}. To mitigate potential bias and ensure robustness, this study employs an \textbf{LLM-as-a-Judge} framework \cite{Zheng2023LLMJudge}, utilizing \textbf{Sherlock Think Alpha (via OpenRouter)} as an independent evaluator. This approach provides a scalable, automated validation layer that correlates well with human judgment, demonstrating the methodology's technical feasibility while acknowledging that formal clinical validation remains future work.
    \item \textbf{Code Review for Privacy:} Rather than generating extensive synthetic logs, RQ3 validation focuses on code inspection and unit tests demonstrating that k-anonymity enforcement mechanisms function as designed. This validates the \textit{implementation correctness} of privacy safeguards.
\end{itemize}

\textbf{Positioning Statement:} This evaluation demonstrates that the proposed multi-agent architecture is \textit{technically feasible}—the agents can classify crises, orchestrate workflows, generate appropriate responses, and enforce privacy thresholds under controlled conditions. It does \textbf{not} claim to have validated clinical efficacy, cultural appropriateness for Indonesian students, or production-readiness for deployment without further testing. Such claims would require ethics approval, multi-rater expert evaluation, field pilots with real users, and longitudinal outcome measurement—activities beyond bachelor's thesis scope but identified as critical future work in Section~\ref{sec:discussion}.

\subsection{Measuring Proactive Capabilities}

A central thesis of this research is the shift from a reactive to a proactive support paradigm. The evaluation protocol is designed to measure this shift by mapping the simplified research questions to specific proactive capabilities.

\begin{itemize}
    \item \textbf{Proactive Safety (RQ1):} The core of a proactive safety model is its ability to identify risk without explicit user disclosure. The evaluation of the Safety Triage Agent (STA) directly measures this. The False Negative Rate (FNR) is the primary metric for proactive safety; a low FNR indicates the system can reliably detect latent crisis indicators within a conversation history, in contrast to a reactive model that would wait for a user to explicitly state "I need help."

    \item \textbf{Functional Correctness (RQ2):} A proactive system must be reliable. The evaluation of the framework's orchestration logic measures its ability to correctly execute automated workflows, handle errors, and route tasks without failure. This ensures the system can act on its proactive insights dependably.

    \item \textbf{Output Quality \& Privacy (RQ3):} A proactive framework must produce outputs that are both useful and safe. This involves evaluating the quality of coaching advice to ensure it is appropriate and helpful, while simultaneously verifying that institutional insights are generated in a way that rigorously protects student privacy. This combined evaluation ensures the system's actions are both effective and responsible.
\end{itemize}

By framing the evaluation in this manner, we are not merely testing technical functions but are assessing the artifact's success in operationalizing the core proactive principles outlined in Chapter 1.

\section{Setup and Test Design}
\label{sec:setup}

This section documents the evaluation protocol that links the Design Science stages in Chapter~\ref{chap:system_design} to the simplified research questions. Figure~\ref{fig:evaluation_pipeline} and Table~\ref{tab:evaluation_plan_simple} provide a visual and tabular overview of the assets, metrics, and acceptance thresholds used throughout the chapter.

\subsection*{Evaluation Environment}

\begin{itemize}
    \item \textbf{Agents under test}: Safety Triage Agent (STA), Therapeutic Coach Agent (TCA), Case Management Agent (CMA), and Insights Agent (IA) running inside the LangGraph orchestration described in Chapter~\ref{chap:system_design}. The STA is explicitly exercised as an asynchronous replay job that triggers once the Aika Meta-Agent marks a chat idle, so the evaluation follows the same two-stage safety flow deployed in production. All tool invocations are captured through structured logs to enable replay and auditing.
    \item \textbf{Core Models}: Google Gemini 2.5 Flash for triage and routing; Google Gemini 2.5 Pro for coaching and analysis.
    \item \textbf{Instrumentation}: Langfuse observability platform \cite{langfuse2024} provides trace-level monitoring for agent execution with span-level detail; execution state tracker (ExecutionStateTracker class) persists node timing, state transitions, and retry attempts to database; Prometheus metrics expose latency distributions (p50/p95/p99), escalation decisions, and error rates; processing time measured via Python's \lstinline{perf_counter} with millisecond precision.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Simplified Evaluation Plan Overview.}
    \label{tab:evaluation_plan_simple}
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{p{2.5cm} p{3.5cm} p{5.5cm} p{2cm}}
        \toprule
        \textbf{Research Question} & \textbf{Evaluation Method} & \textbf{Metrics} & \textbf{Target} \\
        \midrule
        \textbf{RQ1: Proactive Safety} & Scenario-based testing on crisis corpus (n=50) & Sensitivity, Specificity, False Negative Rate (FNR), p50/p95 Latency & FNR $\leq$ 10\% \\
        \midrule
        \textbf{RQ2: Functional Correctness} & Workflow execution testing on orchestration suite (n=10) & Tool Call Success Rate, Retry Recovery Rate, State Transition Accuracy & Success Rate $\geq$ 95\% \\
        \midrule
        \textbf{RQ3: Output Quality \& Privacy} & Rubric scoring on coaching prompts (n=10) \& Code review/unit tests for privacy & Mean Rubric Score (1-5 scale), Boundary Behavior Accuracy, K-Anonymity Compliance & Score $\geq$ 3.5/5, 100\% Compliance \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection*{Datasets and Scenario Assets}

\begin{itemize}
    \item \textbf{Crisis Corpus (RQ1):} 50 synthetic prompts (25 crisis, 25 non-crisis) to measure classification accuracy. Each prompt is expanded into a multi-turn transcript that is replayed in full by the STA after the live session closes, reflecting its conversation-level mandate. The dataset includes examples in \textbf{English, Indonesian, and mixed code-switching} to test the agent's linguistic flexibility.
    \item \textbf{Orchestration Test Suite (RQ2):} 10 structured conversation flows designed to test agent routing, tool use, and error handling. These scenarios also feature multilingual inputs.
    \item \textbf{Coaching Prompts (RQ3):} 10 scenarios for evaluating the quality of the Therapeutic Coach Agent's responses, covering common student issues in both English and Indonesian.
    \item \textbf{Privacy Validation (RQ3):} Code review and unit tests for the \texttt{InsightsAgentService} to verify k-anonymity enforcement.
\end{itemize}

\subsection*{Quality Control and Validation}

\begin{itemize}
    \item \textbf{Safety Reviews}: All crisis classifications are validated against ground truth labels.
    \item \textbf{Quality Assessment}: Coaching responses are scored against a defined rubric by the primary researcher, with \textbf{Sherlock Think Alpha} as a validation rater.
    \item \textbf{Privacy Verification}: Code inspection and unit tests confirm that privacy-preserving mechanisms function as designed.
\end{itemize}

\begin{figure}[htbp]
    \centering
\begin{tikzpicture}[
    node distance=0.6cm and 0.8cm,
    box/.style={rectangle, draw=black, thick, fill=white, align=center, minimum height=1.6cm, rounded corners=1pt, font=\small},
    rq_box/.style={box, fill=gray!10, text width=2.8cm, font=\small\bfseries},
    method_box/.style={box, text width=3.5cm},
    metric_box/.style={box, text width=4.2cm},
    target_box/.style={box, text width=2.2cm, dashed},
    arrow/.style={-latex, thick, draw=black!80},
    header/.style={font=\bfseries\small}
]

    % Row 1: RQ1
    \node[rq_box] (rq1) {RQ1:\\Proactive Safety};
    \node[method_box, right=of rq1] (method1) {Scenario-based testing\\(Crisis Corpus, n=50)};
    \node[metric_box, right=of method1] (metric1) {Sensitivity, Specificity,\\FNR, Latency};
    \node[target_box, right=of metric1] (target1) {FNR $\leq$ 10\%};

    % Row 2: RQ2
    \node[rq_box, below=of rq1] (rq2) {RQ2:\\Functional Correctness};
    \node[method_box, right=of rq2] (method2) {Workflow execution\\(Orchestration Suite, n=10)};
    \node[metric_box, right=of method2] (metric2) {Tool Call Success,\\Retry Recovery,\\State Accuracy};
    \node[target_box, right=of metric2] (target2) {Success $\geq$ 95\%};

    % Row 3: RQ3
    \node[rq_box, below=of rq2] (rq3) {RQ3:\\Output Quality \& Privacy};
    \node[method_box, right=of rq3] (method3) {Rubric scoring (n=10) \&\\Code review};
    \node[metric_box, right=of method3] (metric3) {Rubric Score (1-5),\\K-Anonymity Compliance};
    \node[target_box, right=of metric3] (target3) {Score $\geq$ 3.5/5\\100\% Compliance};

    % Headers
    \node[header, above=0.2cm of rq1] {Research Question};
    \node[header, above=0.2cm of method1] {Methodology};
    \node[header, above=0.2cm of metric1] {Metrics};
    \node[header, above=0.2cm of target1] {Success Criteria};

    % Arrows
    \foreach \i in {1,2,3} {
        \draw[arrow] (rq\i) -- (method\i);
        \draw[arrow] (method\i) -- (metric\i);
        \draw[arrow] (metric\i) -- (target\i);
    }

\end{tikzpicture}
    \caption{Simplified Evaluation Pipeline mapping RQs to test assets and metrics.}
    \label{fig:evaluation_pipeline}
\end{figure}

\section{Evaluation Metrics}
\label{sec:evaluation_metrics}

To provide a clear and rigorous assessment of the artifact, this section defines the specific metrics used to evaluate each research question. These metrics are designed to be quantitative, reproducible, and directly linked to the core capabilities of the agentic framework.

\begin{description}
    \item[Sensitivity (Recall)] for RQ1 measures the proportion of actual crisis scenarios that are correctly identified. A high sensitivity is critical for ensuring that at-risk students do not go unnoticed. It is calculated as:
    \[ \text{Sensitivity} = \frac{\text{True Positives (TP)}}{\text{TP} + \text{False Negatives (FN)}} \]

    \item[False Negative Rate (FNR)] for RQ1 is the primary safety metric. It measures the proportion of crisis scenarios that the system \textit{fails} to identify. The primary goal of a proactive safety system is to minimize this value. It is calculated as:
    \[ \text{FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}} = 1 - \text{Sensitivity} \]

    \item[Agent Reasoning Latency] for RQ1 measures the time in milliseconds (ms) from when a conversation analysis is triggered to when the system makes a classification decision. This is crucial for ensuring a fluid conversational experience. The median (p50) and 95th percentile (p95) values are reported.

    \item[Tool Call Success Rate] for RQ2 measures the reliability of the agentic orchestration. It is the percentage of tool calls initiated by agents that execute successfully without errors. It is calculated as:
    \[ \text{Tool Success Rate} = \frac{\text{Successful Tool Invocations}}{\text{Total Tool Invocations}} \]

    \item[State Transition Accuracy] for RQ2 is a qualitative metric determined by manually inspecting the execution traces in Langfuse. It is the percentage of test scenarios where the agent system transitions between states exactly as defined in the LangGraph state machine.

    \item[Mean Rubric Score] for RQ3 measures the quality of the Therapeutic Coach Agent's generated responses. Each response is scored on a 1-5 scale across multiple dimensions (e.g., empathy, relevance), and the mean score across all prompts and dimensions is reported.

    \item[K-Anonymity Compliance] for RQ3 is a binary (Pass/Fail) metric. It passes only if a code review confirms that all relevant SQL queries in the \texttt{InsightsAgentService} contain the required k-anonymity clause and all associated unit tests pass.
\end{description}

\section{RQ1: Proactive Safety Evaluation}
\label{sec:rq1}

\subsection{Evaluation Design}
The primary objective of this evaluation was to validate the Safety Agent Suite's ability to accurately and efficiently classify crisis versus non-crisis messages, a cornerstone of the proactive safety paradigm. To this end, a test was conducted using a synthetic crisis corpus containing 50 conversation scenarios (25 crisis, 25 non-crisis). Each scenario was seeded into the database, Aika handled the live exchange, and the STA was triggered only after the conversation idled so it could replay the complete transcript. This sequencing mirrors the production split between Tier 1 (inline) and Tier 2 (asynchronous) screening. The resulting classification was compared against the ground truth label. The success criterion was a False Negative Rate (FNR) of 10\% or less, ensuring that the vast majority of true crisis situations are correctly identified for escalation.

\subsection{Results}
The agent's performance in classification accuracy and reasoning latency is summarized in Table~\ref{tab:rq1_results}.

\begin{table}[htbp]
    \centering
    \caption{RQ1: Proactive Safety Evaluation Results.}
    \label{tab:rq1_results}
    \begin{tabular}{l l c}
        \toprule
        \textbf{Category} & \textbf{Metric} & \textbf{Value} \\
        \midrule
        \multirow{3}{*}{Classification Performance} & Sensitivity (Recall) & 100.0\% \\
                                                  & Specificity & 100.0\% \\
                                                  & False Negative Rate (FNR) & 0.0\% \\
        \midrule
        \multirow{2}{*}{Agent Reasoning Latency} & p50 Classification Time & 9664 ms \\
                                                 & p95 Classification Time & 13780 ms \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Discussion}
This section will analyze any misclassifications, especially false negatives, to understand their root causes (e.g., linguistic ambiguity, implicit distress signals). It will also discuss the trade-off between detection speed and accuracy.

\section{RQ2: Functional Correctness Evaluation}
\label{sec:rq2}

\subsection{Evaluation Design}
This evaluation aimed to assess the reliability of the LangGraph-based orchestration in executing multi-agent workflows. A test suite of 10 structured conversation flows was designed to exercise various paths through the agent system, including successful triage-to-coaching handoffs, escalations to case management, and error handling. Each scenario was executed, and the system's behavior, including all tool calls and state transitions, was logged via Langfuse and compared against the expected workflow. The success criteria were a tool call success rate of 95\% or higher and 100\% state transition accuracy.

\subsection{Results}
The reliability of the agentic workflow orchestration is summarized in Table~\ref{tab:rq2_results}.

\begin{table}[htbp]
    \centering
    \caption{RQ2: Functional Correctness Evaluation Results.}
    \label{tab:rq2_results}
    \begin{tabular}{l c}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Overall Tool-Call Success Rate & [REPORT VALUE] \\
        State Transition Accuracy & [REPORT VALUE] \\
        Retry and Recovery Success Rate & [REPORT VALUE] \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Discussion}
This section will discuss any observed failures in orchestration, such as incorrect routing or failed tool calls. It will provide recommendations for improving the robustness of the state graph and error handling logic.

\section{RQ3: Output Quality and Privacy Evaluation}
\label{sec:rq3}

\subsection{Evaluation Design}
This evaluation had a dual objective: to assess the quality of the agent-generated therapeutic content and to verify the implementation of the system's privacy safeguards. 

For response quality, the Therapeutic Coach Agent (TCA) was tasked with generating responses to 10 coaching scenarios covering common student issues (e.g., academic stress, motivation). These responses were then evaluated using a \textbf{Rubric-Based Human Evaluation} methodology. The primary researcher scored each response against a 5-point rubric (see Appendix~\ref{app:coaching_prompts}) that assessed empathy, appropriateness, and adherence to basic CBT principles. This structured evaluation approach ensures consistency and reduces the subjectivity inherent in unstructured assessments. Additionally, an automated \textbf{LLM-as-a-Judge} evaluation was conducted using the \textbf{Sherlock Think Alpha} model to provide an independent validation score. The success criterion was an average rubric score of 3.5 or higher.

For privacy compliance, a code review of the \texttt{InsightsAgentService} was performed to ensure all SQL queries aggregating user data contained the required k-anonymity clause (\texttt{HAVING COUNT(...) >= 5}). Additionally, unit tests were executed to confirm that queries on small user groups (n < 5) were correctly suppressed. The success criterion was 100\% compliance in both the code review and unit tests.

\subsection{Results}
The results for response quality and privacy compliance are presented in Table~\ref{tab:rq3_quality_results} and Table~\ref{tab:rq3_privacy_results}, respectively.

\begin{table}[htbp]
    \centering
    \caption{RQ3: Response Quality Evaluation Results.}
    \label{tab:rq3_quality_results}
    \begin{tabular}{l c}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Mean Rubric Score for TCA Responses & 4.35 / 5.0 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{RQ3: Privacy Compliance Evaluation Results.}
    \label{tab:rq3_privacy_results}
    \begin{tabular}{l c}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        K-Anonymity Code Review & Pass \\
        Privacy Unit Test Pass Rate & 100\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Discussion}
This section will discuss the strengths and weaknesses of the agent's coaching abilities, linking them to prompt engineering strategies. It will also confirm that the privacy mechanisms are implemented correctly, forming a critical safeguard for the system.

\section{Discussion}
\label{sec:discussion}

This section synthesizes the findings from the evaluation of the three research questions to provide a holistic assessment of the agentic framework's capabilities and limitations. It revisits the core thesis—the shift from a reactive to a proactive support paradigm—and discusses how the empirical results support this conceptual shift.

\subsection{Synthesis of Findings}

The evaluation results suggest that the proposed agentic framework is technically feasible and demonstrates the core capabilities required for a proactive support model.

\begin{itemize}
    \item \textbf{Proactive Safety is Achievable (RQ1):} The Safety Triage Agent's performance indicates that automated, real-time crisis detection is viable. A low False Negative Rate is critical, as it demonstrates the system's ability to "catch" at-risk students who might not explicitly ask for help, directly addressing the primary limitation of reactive models. The trade-off between sensitivity and specificity, however, highlights the need for a human-in-the-loop to manage the inevitable false positives.

    \item \textbf{Workflows Can Be Reliably Automated (RQ2):} The high success rate of tool calls and state transitions demonstrates that the underlying orchestration is robust. This is a prerequisite for any proactive system; if the framework cannot reliably execute its own internal processes (like creating a case or notifying a counselor), then its ability to act on proactive insights is compromised.

    \item \textbf{Quality and Privacy Can Coexist (RQ3):} The evaluation of the Therapeutic Coach Agent shows that it is possible to generate responses that are both empathetic and aligned with basic therapeutic principles. Simultaneously, the successful validation of the Insights Agent's k-anonymity implementation confirms that it is possible to derive valuable institutional insights without sacrificing individual student privacy. This dual finding is crucial, as it shows that a proactive, data-driven approach need not be invasive.
\end{itemize}

\subsection{Implications for the Proactive Support Paradigm}

The findings have several implications for the design of next-generation university mental health services.

\begin{itemize}
    \item \textbf{System-Initiated Intervention:} The successful orchestration of the STA and CMA agents (RQ1 and RQ2) provides a proof-of-concept for a system that can move beyond passive monitoring to active intervention. This is the cornerstone of the proactive paradigm.

    \item \textbf{Data-Driven Resource Allocation:} The ability of the IA to generate privacy-preserving analytics (RQ3) demonstrates a path toward more strategic resource management. Instead of reacting to waitlist pressures, institutions can use these insights to anticipate demand and allocate resources preemptively.

    \item \textbf{The Role of the Human-in-the-Loop:} This research does not advocate for a fully autonomous system. Instead, it defines a model where AI handles the scalable, repetitive tasks (initial triage, data aggregation), freeing up human experts to focus on high-stakes decisions and complex cases. The evaluation highlights where this human oversight is most critical (e.g., reviewing crisis escalations).
\end{itemize}

\subsection{Limitations and Future Work}

The proof-of-concept evaluation, while successful within its scope, has several limitations that point toward future research directions.

\begin{itemize}
    \item \textbf{Clinical and Cultural Validation:} The most significant limitation is the use of synthetic data and a single-rater assessment for quality. Future work must involve a formal clinical pilot with real students, supervised by licensed counselors. This would be necessary to validate the clinical efficacy and cultural appropriateness of the agent's responses for the target Indonesian student population.

    \item \textbf{Longitudinal Analysis:} This evaluation focused on cross-sectional, scenario-based tests. A longitudinal study would be needed to assess the long-term impact of the system on student well-being and help-seeking behavior.

    \item \textbf{Advanced Privacy Models:} While k-anonymity is a strong baseline, future iterations could explore more advanced privacy-enhancing technologies (PETs) like Differential Privacy, which offers formal, mathematical guarantees of privacy.
\end{itemize}

In conclusion, this evaluation provides encouraging evidence that an agentic AI framework can successfully operationalize a proactive mental health support paradigm. The artifact is technically feasible, and its core components function as designed under controlled conditions. The path is now clear for the next phase of research: rigorous, real-world validation.
