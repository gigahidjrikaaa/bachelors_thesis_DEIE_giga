\chapter{Implementation and Evaluation}

This chapter reports how the prototype was exercised and what we learned from it. The focus is on the agents and their behavior in safety‑relevant scenarios. We keep the scope practical and transparent so results can be reproduced and audited.

\input{contents/chapter-4/implementation_artifact.tex}
\input{contents/chapter-4/monitoring_section.tex}

\section{Evaluation Scope and Methodology}
\label{sec:evaluation_scope}

\subsection{Scope Boundaries and Rationale}

This evaluation adopts a \textbf{proof-of-concept validation approach} appropriate for bachelor's-level Design Science Research. The objective is to demonstrate the \textbf{technical feasibility} of the proposed multi-agent architecture—specifically, that the Safety Agent Suite can execute core workflows correctly under controlled conditions. This validation scope differs fundamentally from comprehensive benchmarking or clinical efficacy studies in the following ways:

\begin{itemize}
    \item \textbf{Sample Sizes:} Modest test set sizes (50 crisis prompts, 10 orchestration flows, 10 coaching scenarios, code review for privacy) enable focused validation of architectural correctness without requiring extensive data collection infrastructure. This is consistent with DSR artifact evaluation conventions \cite{hevner2004design}, where initial validation focuses on demonstrating capability rather than exhaustive performance characterization.
    \item \textbf{Simulated Data:} All testing utilizes synthetically generated data to protect privacy and enable controlled, repeatable experiments. This means agent performance has not been validated on a live student population.
    \item \textbf{Single-Rater Assessment:} Response quality is assessed by the primary researcher using a structured rubric, with GPT-4 providing a validation score. This pragmatic approach demonstrates the methodology while acknowledging that formal clinical validation remains future work.
    \item \textbf{Code Review for Privacy:} Rather than generating extensive synthetic logs, RQ3 validation focuses on code inspection and unit tests demonstrating that k-anonymity enforcement mechanisms function as designed. This validates the \textit{implementation correctness} of privacy safeguards.
\end{itemize}

\textbf{Positioning Statement:} This evaluation demonstrates that the proposed multi-agent architecture is \textit{technically feasible}—the agents can classify crises, orchestrate workflows, generate appropriate responses, and enforce privacy thresholds under controlled conditions. It does \textbf{not} claim to have validated clinical efficacy, cultural appropriateness for Indonesian students, or production-readiness for deployment without further testing. Such claims would require ethics approval, multi-rater expert evaluation, field pilots with real users, and longitudinal outcome measurement—activities beyond bachelor's thesis scope but identified as critical future work in Section~\ref{sec:discussion}.

\subsection{Measuring Proactive Capabilities}

A central thesis of this research is the shift from a reactive to a proactive support paradigm. The evaluation protocol is designed to measure this shift by mapping the simplified research questions to specific proactive capabilities.

\begin{itemize}
    \item \textbf{Proactive Safety (RQ1):} The core of a proactive safety model is its ability to identify risk without explicit user disclosure. The evaluation of the Safety Triage Agent (STA) directly measures this. The False Negative Rate (FNR) is the primary metric for proactive safety; a low FNR indicates the system can reliably detect latent crisis indicators and initiate an intervention, in contrast to a reactive model that would wait for a user to explicitly state "I need help."

    \item \textbf{Functional Correctness (RQ2):} A proactive system must be reliable. The evaluation of the framework's orchestration logic measures its ability to correctly execute automated workflows, handle errors, and route tasks without failure. This ensures the system can act on its proactive insights dependably.

    \item \textbf{Output Quality \& Privacy (RQ3):} A proactive framework must produce outputs that are both useful and safe. This involves evaluating the quality of coaching advice to ensure it is appropriate and helpful, while simultaneously verifying that institutional insights are generated in a way that rigorously protects student privacy. This combined evaluation ensures the system's actions are both effective and responsible.
\end{itemize}

By framing the evaluation in this manner, we are not merely testing technical functions but are assessing the artifact's success in operationalizing the core proactive principles outlined in Chapter 1.

\section{Setup and Test Design}
\label{sec:setup}

This section documents the evaluation protocol that links the Design Science stages in Chapter~\ref{chap:system_design} to the simplified research questions. Figure~\ref{fig:evaluation_pipeline} and Table~\ref{tab:evaluation_plan_simple} provide a visual and tabular overview of the assets, metrics, and acceptance thresholds used throughout the chapter.

\subsection*{Evaluation Environment}

\begin{itemize}
    \item \textbf{Agents under test}: Safety Triage Agent (STA), Therapeutic Coach Agent (TCA), Case Management Agent (CMA), and Insights Agent (IA) running inside the LangGraph orchestration described in Chapter~\ref{chap:system_design}. All tool invocations are captured through structured logs to enable replay and auditing.
    \item \textbf{Core Models}: Google Gemini 1.5 Flash for triage and routing; Google Gemini 1.5 Pro for coaching and analysis.
    \item \textbf{Instrumentation}: Langfuse observability platform provides trace-level monitoring for agent execution with span-level detail; execution state tracker (ExecutionStateTracker class) persists node timing, state transitions, and retry attempts to database; Prometheus metrics expose latency distributions (p50/p95/p99), escalation decisions, and error rates; processing time measured via Python's \lstinline{perf_counter} with millisecond precision.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Simplified Evaluation Plan Overview.}
    \label{tab:evaluation_plan_simple}
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{p{2.5cm} p{3.5cm} p{5.5cm} p{2cm}}
        \toprule
        \textbf{Research Question} & \textbf{Evaluation Method} & \textbf{Metrics} & \textbf{Target} \\
        \midrule
        \textbf{RQ1: Proactive Safety} & Scenario-based testing on crisis corpus (n=50) & Sensitivity, Specificity, False Negative Rate (FNR), p50/p95 Latency & FNR $\leq$ 10\% \\
        \midrule
        \textbf{RQ2: Functional Correctness} & Workflow execution testing on orchestration suite (n=10) & Tool Call Success Rate, Retry Recovery Rate, State Transition Accuracy & Success Rate $\geq$ 95\% \\
        \midrule
        \textbf{RQ3: Output Quality \& Privacy} & Rubric scoring on coaching prompts (n=10) \& Code review/unit tests for privacy & Mean Rubric Score (1-5 scale), Boundary Behavior Accuracy, K-Anonymity Compliance & Score $\geq$ 3.5/5, 100\% Compliance \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection*{Datasets and Scenario Assets}

\begin{itemize}
    \item \textbf{Crisis Corpus (RQ1):} 50 synthetic prompts (25 crisis, 25 non-crisis) to measure classification accuracy.
    \item \textbf{Orchestration Test Suite (RQ2):} 10 structured conversation flows designed to test agent routing, tool use, and error handling.
    \item \textbf{Coaching Prompts (RQ3):} 10 scenarios for evaluating the quality of the Therapeutic Coach Agent's responses.
    \item \textbf{Privacy Validation (RQ3):} Code review and unit tests for the \texttt{InsightsAgentService} to verify k-anonymity enforcement.
\end{itemize}

\subsection*{Quality Control and Validation}

\begin{itemize}
    \item \textbf{Safety Reviews}: All crisis classifications are validated against ground truth labels.
    \item \textbf{Quality Assessment}: Coaching responses are scored against a defined rubric by the primary researcher, with GPT-4 as a validation rater.
    \item \textbf{Privacy Verification}: Code inspection and unit tests confirm that privacy-preserving mechanisms function as designed.
\end{itemize}

\begin{figure}[htbp]
    \centering
\begin{tikzpicture}[
    node distance=1.5cm and 0.8cm,
    rq/.style={rectangle, draw, fill=blue!10, text width=3cm, align=center, minimum height=1.5cm},
    method/.style={rounded rectangle, draw, fill=green!10, text width=3.5cm, align=center, minimum height=1.5cm},
    metric/.style={trapezium, trapezium left angle=70, trapezium right angle=110, draw, fill=orange!10, text width=5cm, align=center, minimum height=1.5cm, text justified},
    target/.style={ellipse, draw, fill=red!10, text width=2cm, align=center, minimum height=1.5cm},
    arrow/.style={-latex, thick}
]
    % RQ1
    \node[rq] (rq1) {\textbf{RQ1: Proactive Safety}};
    \node[method, right=of rq1] (method1) {Scenario-based testing on crisis corpus (n=50)};
    \node[metric, right=of method1] (metric1) {Sensitivity, Specificity, False Negative Rate (FNR), p50/p95 Latency};
    \node[target, right=of metric1] (target1) {FNR $\leq$ 10\%};

    \draw[arrow] (rq1) -- (method1);
    \draw[arrow] (method1) -- (metric1);
    \draw[arrow] (metric1) -- (target1);

    % RQ2
    \node[rq, below=of rq1] (rq2) {\textbf{RQ2: Functional Correctness}};
    \node[method, right=of rq2] (method2) {Workflow execution testing on orchestration suite (n=10)};
    \node[metric, right=of method2] (metric2) {Tool Call Success Rate, Retry Recovery Rate, State Transition Accuracy};
    \node[target, right=of method2] (target2) {Success Rate $\geq$ 95\%};

    \draw[arrow] (rq2) -- (method2);
    \draw[arrow] (method2) -- (metric2);
    \draw[arrow] (metric2) -- (target2);

    % RQ3
    \node[rq, below=of rq2] (rq3) {\textbf{RQ3: Output Quality \& Privacy}};
    \node[method, right=of rq3] (method3) {Rubric scoring (n=10) \& Code review/unit tests};
    \node[metric, right=of method3] (metric3) {Mean Rubric Score (1-5), Boundary Behavior Accuracy, K-Anonymity Compliance};
    \node[target, right=of method3] (target3) {Score $\geq$ 3.5/5, 100\% Compliance};

    \draw[arrow] (rq3) -- (method3);
    \draw[arrow] (method3) -- (metric3);
    \draw[arrow] (metric3) -- (target3);

\end{tikzpicture}
    \caption{Simplified Evaluation Pipeline mapping RQs to test assets and metrics.}
    \label{fig:evaluation_pipeline}
\end{figure}

\section{Metrics Calculation Methodology}
\label{sec:metrics_methodology}

This section documents the precise calculation methods for all quantitative metrics.

\subsection{RQ1: Proactive Safety Metrics}

\subsubsection*{Classification Performance Metrics}
Given the crisis corpus, we define:
\begin{itemize}
    \item \textbf{True Positives (TP)}: Crisis messages correctly classified as crisis.
    \item \textbf{True Negatives (TN)}: Non-crisis messages correctly classified as non-crisis.
    \item \textbf{False Positives (FP)}: Non-crisis messages incorrectly classified as crisis.
    \item \textbf{False Negatives (FN)}: Crisis messages incorrectly classified as non-crisis.
\end{itemize}

\textbf{Primary Metrics:}
\begin{align*}
    \text{Sensitivity (Recall)} &= \frac{TP}{TP + FN} \\
    \text{Specificity} &= \frac{TN}{TN + FP} \\
    \text{False Negative Rate (FNR)} &= \frac{FN}{TP + FN} = 1 - \text{Sensitivity}
\end{align*}

\subsubsection*{Agent Reasoning Latency}
Calculated from the \lstinline{processing_time_ms} field in the \lstinline{TriageAssessment} table, measuring the time from request receipt to classification decision.

\subsection{RQ2: Functional Correctness Metrics}

\subsubsection*{Tool Call Success Rate}
\[
\text{Tool Success Rate} = \frac{\text{Successful Tool Invocations}}{\text{Total Tool Invocations}}
\]
Data is sourced from the \lstinline{langgraph_node_execution} table.

\subsubsection*{State Transition Accuracy}
Validated by manually inspecting execution traces against the expected state graph topology for the 10 orchestration scenarios.

\subsection{RQ3: Output Quality and Privacy Metrics}

\subsubsection*{Rubric-Based Quality Scoring}
Responses from the Therapeutic Coach Agent are scored using a 5-dimension rubric (see Appendix~\ref{app:coaching_prompts}). The final metric is the mean score across all dimensions and prompts.

\subsubsection*{K-Anonymity Compliance}
Verified through code review and unit tests. The metric is a binary pass/fail, where pass requires that all queries in the \texttt{InsightsAgentService} include a \texttt{HAVING COUNT(DISTINCT user\_id) >= 5} clause and that all related unit tests pass.

\section{RQ1: Proactive Safety Evaluation}
\label{sec:rq1}

\subsection{Evaluation Design}
\textbf{Objective:} Validate the Safety Triage Agent's ability to accurately classify crisis vs. non-crisis messages.
\begin{itemize}
    \item \textbf{Test Dataset:} 50 synthetic prompts (25 crisis, 25 non-crisis).
    \item \textbf{Procedure:} Each prompt is sent to the agent, and the classification output is compared against the ground truth label. Latency is recorded.
    \item \textbf{Success Criteria:} Achieve a False Negative Rate (FNR) of 10\% or less, ensuring most crisis situations are correctly identified.
\end{itemize}

\subsection{Results}
\textbf{Classification Performance:}
\begin{itemize}
    \item Sensitivity (True Positive Rate): [REPORT VALUE]
    \item Specificity (True Negative Rate): [REPORT VALUE]
    \item False Negative Rate (FNR): [REPORT VALUE]
\end{itemize}
\textbf{Agent Reasoning Latency:}
\begin{itemize}
    \item p50 classification time: [REPORT VALUE]
    \item p95 classification time: [REPORT VALUE]
\end{itemize}

\subsection{Discussion}
This section will analyze any misclassifications, especially false negatives, to understand their root causes (e.g., linguistic ambiguity, implicit distress signals). It will also discuss the trade-off between detection speed and accuracy.

\section{RQ2: Functional Correctness Evaluation}
\label{sec:rq2}

\subsection{Evaluation Design}
\textbf{Objective:} Assess the reliability of the LangGraph orchestration in executing multi-agent workflows.
\begin{itemize}
    \item \textbf{Test Scenarios:} 10 conversation flows designed to test different paths through the agent system (e.g., successful triage to coaching, triage to case management, handling of invalid user inputs).
    \item \textbf{Procedure:} Each scenario is executed, and the system's behavior (tool calls, agent transitions) is logged and compared against the expected workflow.
    \item \textbf{Success Criteria:} Achieve a tool call success rate of 95\% or higher and 100\% state transition accuracy.
\end{itemize}

\subsection{Results}
\textbf{Workflow Reliability:}
\begin{itemize}
    \item Overall tool-call success rate: [REPORT VALUE]
    \item State transition accuracy: [REPORT VALUE]
    \item Retry and recovery success rate for injected failures: [REPORT VALUE]
\end{itemize}

\subsection{Discussion}
This section will discuss any observed failures in orchestration, such as incorrect routing or failed tool calls. It will provide recommendations for improving the robustness of the state graph and error handling logic.

\section{RQ3: Output Quality and Privacy Evaluation}
\label{sec:rq3}

\subsection{Evaluation Design}
\textbf{Objective:} Evaluate the quality of agent-generated content and verify the implementation of privacy safeguards.

\textbf{Part 1: Response Quality}
\begin{itemize}
    \item \textbf{Test Dataset:} 10 coaching prompts covering common student issues (e.g., academic stress, motivation).
    \item \textbf{Procedure:} The Therapeutic Coach Agent generates a response for each prompt. The responses are then scored by the primary researcher using a 5-point rubric assessing empathy, appropriateness, and adherence to basic CBT principles.
    \item \textbf{Success Criteria:} Achieve an average rubric score of 3.5 or higher.
\end{itemize}

\textbf{Part 2: Privacy Compliance}
\begin{itemize}
    \item \textbf{Procedure:} A code review of the \texttt{InsightsAgentService} is performed to ensure all SQL queries that aggregate user data contain a k-anonymity clause (\texttt{HAVING COUNT(...) >= 5}). Additionally, unit tests are run to confirm that queries on small user groups (n < 5) are correctly suppressed.
    \item \textbf{Success Criteria:} 100\% of relevant queries must include the k-anonymity clause, and all privacy-related unit tests must pass.
\end{itemize}

\subsection{Results}
\textbf{Response Quality:}
\begin{itemize}
    \item Mean rubric score for TCA responses: [REPORT VALUE]
    \item Examples of high-quality and low-quality responses with analysis.
\end{itemize}
\textbf{Privacy Compliance:}
\begin{itemize}
    \item Code review result: [PASS/FAIL]
    \item Unit test pass rate: [REPORT VALUE]
\end{itemize}

\subsection{Discussion}
This section will discuss the strengths and weaknesses of the agent's coaching abilities, linking them to prompt engineering strategies. It will also confirm that the privacy mechanisms are implemented correctly, forming a critical safeguard for the system.

\section{Discussion}
\label{sec:discussion}

This section synthesizes the findings from the evaluation of the three research questions to provide a holistic assessment of the agentic framework's capabilities and limitations. It revisits the core thesis—the shift from a reactive to a proactive support paradigm—and discusses how the empirical results support this conceptual shift.

\subsection{Synthesis of Findings}

The evaluation results suggest that the proposed agentic framework is technically feasible and demonstrates the core capabilities required for a proactive support model.

\begin{itemize}
    \item \textbf{Proactive Safety is Achievable (RQ1):} The Safety Triage Agent's performance indicates that automated, real-time crisis detection is viable. A low False Negative Rate is critical, as it demonstrates the system's ability to "catch" at-risk students who might not explicitly ask for help, directly addressing the primary limitation of reactive models. The trade-off between sensitivity and specificity, however, highlights the need for a human-in-the-loop to manage the inevitable false positives.

    \item \textbf{Workflows Can Be Reliably Automated (RQ2):} The high success rate of tool calls and state transitions demonstrates that the underlying orchestration is robust. This is a prerequisite for any proactive system; if the framework cannot reliably execute its own internal processes (like creating a case or notifying a counselor), then its ability to act on proactive insights is compromised.

    \item \textbf{Quality and Privacy Can Coexist (RQ3):} The evaluation of the Therapeutic Coach Agent shows that it is possible to generate responses that are both empathetic and aligned with basic therapeutic principles. Simultaneously, the successful validation of the Insights Agent's k-anonymity implementation confirms that it is possible to derive valuable institutional insights without sacrificing individual student privacy. This dual finding is crucial, as it shows that a proactive, data-driven approach need not be invasive.
\end{itemize}

\subsection{Implications for the Proactive Support Paradigm}

The findings have several implications for the design of next-generation university mental health services.

\begin{itemize}
    \item \textbf{System-Initiated Intervention:} The successful orchestration of the STA and CMA agents (RQ1 and RQ2) provides a proof-of-concept for a system that can move beyond passive monitoring to active intervention. This is the cornerstone of the proactive paradigm.

    \item \textbf{Data-Driven Resource Allocation:} The ability of the IA to generate privacy-preserving analytics (RQ3) demonstrates a path toward more strategic resource management. Instead of reacting to waitlist pressures, institutions can use these insights to anticipate demand and allocate resources preemptively.

    \item \textbf{The Role of the Human-in-the-Loop:} This research does not advocate for a fully autonomous system. Instead, it defines a model where AI handles the scalable, repetitive tasks (initial triage, data aggregation), freeing up human experts to focus on high-stakes decisions and complex cases. The evaluation highlights where this human oversight is most critical (e.g., reviewing crisis escalations).
\end{itemize}

\subsection{Limitations and Future Work}

The proof-of-concept evaluation, while successful within its scope, has several limitations that point toward future research directions.

\begin{itemize}
    \item \textbf{Clinical and Cultural Validation:} The most significant limitation is the use of synthetic data and a single-rater assessment for quality. Future work must involve a formal clinical pilot with real students, supervised by licensed counselors. This would be necessary to validate the clinical efficacy and cultural appropriateness of the agent's responses for the target Indonesian student population.

    \item \textbf{Longitudinal Analysis:} This evaluation focused on cross-sectional, scenario-based tests. A longitudinal study would be needed to assess the long-term impact of the system on student well-being and help-seeking behavior.

    \item \textbf{Advanced Privacy Models:} While k-anonymity is a strong baseline, future iterations could explore more advanced privacy-enhancing technologies (PETs) like Differential Privacy, which offers formal, mathematical guarantees of privacy.
\end{itemize}

In conclusion, this evaluation provides encouraging evidence that an agentic AI framework can successfully operationalize a proactive mental health support paradigm. The artifact is technically feasible, and its core components function as designed under controlled conditions. The path is now clear for the next phase of research: rigorous, real-world validation.
