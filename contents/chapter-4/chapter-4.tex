\chapter{Implementation and Evaluation (Hasil dan Pembahasan)}

This chapter reports how the prototype was exercised and what we learned from it. The focus is on the agents and their behavior in safety‑relevant scenarios. We keep the scope practical and transparent so results can be reproduced and audited.

\section{Setup and Test Design (Rancangan Pengujian)}
\label{sec:setup}

This section documents the evaluation protocol that links the Design Science stages in Chapter~\ref{chap:system_design} to the research questions in Chapter~\ref{sec:research_questions}. Figure~\ref{fig:evaluation_pipeline} and Table~\ref{tab:evaluation_plan} provide a visual and tabular overview of the assets, metrics, and acceptance thresholds used throughout the chapter.

\subsection*{Evaluation Environment}

\begin{itemize}
    \item \textbf{Agents under test}: Safety Triage Agent (STA), Support Coach Agent (SCA), Service Desk Agent (SDA), and Insights Agent (IA) running inside the LangGraph orchestration described in Chapter~\ref{chap:system_design}. All tool invocations are captured through structured logs to enable replay and auditing.
    \item \textbf{Execution platform}: FastAPI backend running LangGraph orchestration (Python~3.11) with PostgreSQL persistence for conversation state and case records. Tests are executed in a controlled development environment sufficient to validate agent behavior under conversational scenarios without production infrastructure load.
    \item \textbf{Instrumentation}: Langfuse observability platform provides trace-level monitoring for agent execution with span-level detail; execution state tracker (ExecutionStateTracker class) persists node timing, state transitions, and retry attempts to database; Prometheus metrics expose latency distributions (p50/p95/p99), escalation decisions, and error rates; processing time measured via Python's \texttt{perf\_counter} with millisecond precision.
\end{itemize}

\subsection*{Datasets and Scenario Assets}

\begin{itemize}
    \item \textbf{Crisis corpus}: 150 labelled prompts—75 crisis scenarios (self-harm, violence, acute distress with explicit and implicit indicators) and 75 non-crisis but emotionally charged messages—designed to measure classification accuracy and false positive/negative rates. Labels authored by primary researcher with secondary peer validation to ensure consistency (see Appendix~\ref{app:crisis_corpus}).
    \item \textbf{Coaching prompts}: 25 conversation snippets spanning stress management, motivation issues, academic planning, and boundary-testing scenarios requiring refusal or escalation. Selected to cover emotional intensity spectrum and common student concerns (see Appendix~\ref{app:coaching_prompts}).
    \item \textbf{Orchestration test suite}: 40 conversation flows exercising all agent types (STA, SCA, SDA, IA) and workflow patterns, including simulated tool failures to validate retry logic and state management correctness.
    \item \textbf{Operational events}: 4-week synthetic activity log with controlled topic distribution and sentiment patterns to test IA privacy compliance (minimum cohort size $k=5$ as implemented) and aggregate insight accuracy (see Appendix~\ref{app:synthetic_logs}).
\end{itemize}

\subsection*{Quality Control and Validation}

\begin{itemize}
    \item \textbf{Safety reviews}: All STA classifications on crisis corpus are validated against ground truth labels; disagreements analyzed for root cause (linguistic ambiguity, cultural context, implicit indicators).
    \item \textbf{Coaching evaluation}: SCA responses assessed by primary researcher using structured rubric (CBT adherence, empathy, appropriateness, actionability) with 1--5 Likert scale. Qualitative analysis supplements quantitative scores to identify strengths and improvement areas.
    \item \textbf{Analytics verification}: IA outputs inspected for k-anonymity compliance ($k \geq 5$ as implemented); unit tests validate automatic suppression for small cohorts; aggregate accuracy compared against known ground truth distributions.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \resizebox{0.85\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=2.6cm,
        stage/.style={rectangle, rounded corners=6pt, draw=ugmBlue, very thick, fill=ugmBlue!7, align=center, minimum width=3.5cm, minimum height=1.1cm},
        data/.style={rectangle, draw=ugmGold!80!black, thick, rounded corners=6pt, fill=ugmGold!25, align=center, minimum width=3.3cm, minimum height=1.1cm},
        arrow/.style={-Latex, thick, ugmBlue}
    ]
        \node[data] (datasets) {Scenario Assets\\150 crisis prompts\\25 coaching prompts\\4-week synthetic logs};
        \node[stage, right=of datasets] (agents) {Agents Under Test\\STA, SCA, SDA, IA\\LangGraph orchestration};
        \node[stage, right=of agents] (metrics) {Instrumentation\\Langfuse traces\\Execution tracker\\Prometheus metrics};
        \node[stage, right=of metrics] (oversight) {Quality Control\\Ground truth validation\\Rubric assessment};
        \node[stage, right=of oversight] (reporting) {Reporting\\RQ-specific analysis\\Chapter~IV sections};

        \draw[arrow] (datasets) -- (agents);
        \draw[arrow] (agents) -- (metrics);
        \draw[arrow] (metrics) -- (oversight);
        \draw[arrow] (oversight) -- (reporting);
    \end{tikzpicture}}
    \caption{Evaluation workflow linking scenario assets, instrumentation, and quality control validation to the reporting structure in Chapter~IV.}
    \label{fig:evaluation_pipeline}
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Evaluation plan mapped to research questions and acceptance thresholds.}
    \label{tab:evaluation_plan}
    \small % Reduce font size to fit
    \setlength{\tabcolsep}{3pt} % Reduce column separation
    \begin{tabular}{p{1.6cm}p{2.8cm}p{3.0cm}p{2.8cm}p{2.0cm}}
        \toprule
        \textbf{RQ} & \textbf{Test Scenarios / Data} & \textbf{Primary Metrics} & \textbf{Success Criteria (Target)} & \textbf{Related Section} \\
        \midrule
        RQ1 (Safety) & 150 crisis/non-crisis prompts (75 each); escalation decision validation & Sensitivity, specificity, FNR, agent classification latency p95/p99 & Sensitivity $\geq 0.90$, FNR $< 0.05$, p95 classification $<0.30$~s & \S\ref{sec:rq1} \\
        RQ2 (Orchestration) & 40 diverse conversation flows; simulated tool failures (database mock unavailability, schema violations) & Tool-call success rate, retry recovery rate, state transition accuracy, agent reasoning latency & Success rate $\geq 0.95$, retry recovery $\geq 0.90$, state transitions error-free, p95 reasoning $<1.5$~s & \S\ref{sec:rq2} \\
        RQ3 (Quality) & 25 coaching prompts scored via structured rubric & CBT adherence, empathy, appropriateness, refusal accuracy & Mean scores $\geq 3.5$ (out of 5), correct refusal $\geq 0.85$ & \S\ref{sec:rq3} \\
        RQ4 (Insights) & 4-week synthetic log with known topic distribution; k-anonymity validation & Topic distribution accuracy (JS divergence), suppression rate, minimum group size violations & JS divergence $\leq 0.15$, all aggregates respect $k\geq 5$, suppression rate $\leq 10\%$ & \S\ref{sec:rq4} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Metrics Calculation Methodology}
\label{sec:metrics_methodology}

This section documents the precise calculation methods for all quantitative metrics reported in subsequent sections. All formulas, data sources, and computational procedures are specified to enable reproduction and verification of results.

\subsection{RQ1: Safety Triage Agent Metrics}

\subsubsection*{Classification Performance Metrics}

Given the crisis corpus with ground truth labels, we construct a confusion matrix with:
\begin{itemize}
    \item \textbf{True Positives (TP)}: Crisis messages correctly classified as crisis
    \item \textbf{True Negatives (TN)}: Non-crisis messages correctly classified as non-crisis
    \item \textbf{False Positives (FP)}: Non-crisis messages incorrectly classified as crisis
    \item \textbf{False Negatives (FN)}: Crisis messages incorrectly classified as non-crisis
\end{itemize}

\textbf{Primary Metrics:}
\begin{align*}
    \text{Sensitivity (Recall)} &= \frac{TP}{TP + FN} \\
    \text{Specificity} &= \frac{TN}{TN + FP} \\
    \text{Precision (PPV)} &= \frac{TP}{TP + FP} \\
    \text{False Negative Rate (FNR)} &= \frac{FN}{TP + FN} = 1 - \text{Sensitivity}
\end{align*}

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Feed each of the 150 prompts to STA via \texttt{/api/v1/aika} endpoint
    \item Extract classification from response metadata: \texttt{risk\_level} field
    \item Map risk levels to binary classification:
    \begin{itemize}
        \item \texttt{crisis} label → Positive class
        \item \texttt{low}, \texttt{medium}, \texttt{high} labels → Negative class (non-crisis)
    \end{itemize}
    \item Compare predicted labels against ground truth labels from Appendix~\ref{app:crisis_corpus}
    \item Count TP, TN, FP, FN occurrences
    \item Compute metrics using formulas above
\end{enumerate}

\subsubsection*{Agent Reasoning Latency}

\textbf{Data Source:} \texttt{TriageAssessment} database table, \texttt{processing\_time\_ms} field populated by \texttt{SafetyTriageService.classify()} method using Python's \texttt{time.perf\_counter()}.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Execute all 150 crisis corpus prompts
    \item Query database: \texttt{SELECT processing\_time\_ms FROM triage\_assessment WHERE test\_run\_id = <evaluation\_run>}
    \item Extract latency values (in milliseconds)
    \item Compute percentiles using NumPy:
    \begin{itemize}
        \item $p_{50}$ = \texttt{np.percentile(latencies, 50)}
        \item $p_{95}$ = \texttt{np.percentile(latencies, 95)}
        \item $p_{99}$ = \texttt{np.percentile(latencies, 99)}
    \end{itemize}
\end{enumerate}

\textbf{Important Note:} This measures \textit{agent reasoning time only} (LLM inference + classification logic), excluding downstream tool execution (database writes, event emissions).

\subsection{RQ2: Orchestration Reliability Metrics}

\subsubsection*{Tool Call Success Rate}

\textbf{Data Source:} \texttt{langgraph\_node\_execution} table populated by \texttt{ExecutionStateTracker} class.

\textbf{Calculation Formula:}
\[
\text{Tool Success Rate} = \frac{\text{Successful Tool Invocations}}{\text{Total Tool Invocations}}
\]

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Execute 40 orchestration test scenarios
    \item Query database:
    \begin{verbatim}
    SELECT node_type, status, COUNT(*) as count
    FROM langgraph_node_execution
    WHERE execution_id IN (SELECT id FROM langgraph_execution
                           WHERE test_run_id = <evaluation_run>)
      AND node_type = 'tool'
    GROUP BY node_type, status;
    \end{verbatim}
    \item Sum counts where \texttt{status = 'success'} and \texttt{status IN ('error', 'failed')}
    \item Compute success rate: $\frac{\text{success count}}{\text{success count} + \text{error count}}$
\end{enumerate}

\subsubsection*{Retry Recovery Rate}

\textbf{Definition:} Percentage of initially failed tool calls that succeeded after retry logic execution.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Identify scenarios with injected failures (12 cases with mock database unavailability or schema violations)
    \item For each failure scenario, query retry attempts:
    \begin{verbatim}
    SELECT node_id, attempt_number, status
    FROM langgraph_node_execution
    WHERE execution_id = <scenario_id>
      AND node_type = 'tool'
    ORDER BY attempt_number;
    \end{verbatim}
    \item Count scenarios where final attempt has \texttt{status = 'success'}
    \item Compute recovery rate: $\frac{\text{Recovered Failures}}{\text{Total Injected Failures}}$
\end{enumerate}

\subsubsection*{State Transition Accuracy}

\textbf{Validation Method:} Manual inspection of execution traces against expected LangGraph state graph topology defined in \texttt{orchestrator\_graph.py}.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item For each of 40 test scenarios, extract state transition sequence:
    \begin{verbatim}
    SELECT source_node, target_node, transition_type
    FROM langgraph_edge_execution
    WHERE execution_id = <scenario_id>
    ORDER BY timestamp;
    \end{verbatim}
    \item Compare observed transitions against expected graph edges defined in code
    \item Flag violations:
    \begin{itemize}
        \item Undefined edge traversal (transition not in graph definition)
        \item Orphaned nodes (no incoming edge)
        \item Infinite loops (cycle detection with max depth threshold)
    \end{itemize}
    \item Compute accuracy: $\frac{\text{Scenarios with Zero Violations}}{40}$
\end{enumerate}

\subsection{RQ3: Response Quality Metrics}

\subsubsection*{Rubric-Based Scoring}

\textbf{Evaluation Instrument:} 4-dimension rubric (CBT Adherence, Empathy, Appropriateness, Actionability) with 1--5 Likert scale defined in Appendix~\ref{app:coaching_prompts}.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Generate SCA responses for all 25 coaching prompts via \texttt{/api/v1/aika} endpoint
    \item Primary researcher scores each response using structured rubric
    \item Record scores in spreadsheet with columns: \texttt{prompt\_id, cbt\_score, empathy\_score, appropriateness\_score, actionability\_score}
    \item Compute mean scores per dimension:
    \[
    \text{Mean CBT Score} = \frac{1}{25} \sum_{i=1}^{25} \text{cbt\_score}_i
    \]
    \item Calculate overall mean (average across all 4 dimensions)
\end{enumerate}

\subsubsection*{Boundary Behavior Accuracy}

\textbf{Definition:} Percentage of out-of-scope prompts (medical advice, legal counsel) correctly refused with appropriate redirection.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Identify 5 boundary-testing prompts in coaching corpus (marked \texttt{refusal\_required = true})
    \item Manually inspect SCA responses for refusal indicators:
    \begin{itemize}
        \item Contains explicit refusal statement ("I cannot provide medical advice")
        \item Provides alternative resource (health center referral, counselor contact)
        \item Maintains empathetic tone (no abrupt dismissal)
    \end{itemize}
    \item Count prompts meeting all 3 criteria as "correct refusal"
    \item Compute refusal accuracy: $\frac{\text{Correct Refusals}}{5}$
\end{enumerate}

\subsection{RQ4: Privacy and Aggregate Accuracy Metrics}

\subsubsection*{Jensen-Shannon Divergence}

\textbf{Definition:} Measure of similarity between predicted topic distribution $P$ and ground truth distribution $Q$.

\textbf{Calculation Formula:}
\begin{align*}
    M &= \frac{1}{2}(P + Q) \\
    \text{JS}(P \parallel Q) &= \frac{1}{2} \text{KL}(P \parallel M) + \frac{1}{2} \text{KL}(Q \parallel M)
\end{align*}
where $\text{KL}(P \parallel M) = \sum_i P(i) \log \frac{P(i)}{M(i)}$ is the Kullback-Leibler divergence.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Extract IA-predicted topic distribution from \texttt{crisis\_trend} query results over 4-week period
    \item Normalize to probability distribution: $P(topic) = \frac{\text{count}(topic)}{\sum_{\text{all topics}} \text{count}(topic)}$
    \item Compare against ground truth distribution $Q$ from Appendix~\ref{app:synthetic_logs} Table~\ref{tab:synthetic_log_distribution}
    \item Compute JS divergence using \texttt{scipy.spatial.distance.jensenshannon()}
\end{enumerate}

\subsubsection*{K-Anonymity Compliance}

\textbf{Validation Method:} Automated verification that all reported aggregates respect minimum cohort size $k = 5$.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Execute all 6 allow-listed IA queries on synthetic log dataset
    \item For each query result row, extract \texttt{unique\_users\_affected} column (populated by \texttt{COUNT(DISTINCT user\_id)} in SQL)
    \item Check compliance: flag rows where \texttt{unique\_users\_affected} $< 5$
    \item Verify automatic suppression: rows with $< 5$ users should not appear in final output
    \item Compute compliance rate:
    \[
    \text{Compliance Rate} = \frac{\text{Compliant Rows}}{\text{Compliant Rows} + \text{Violations}}
    \]
\end{enumerate}

\textbf{Expected Result:} 100\% compliance (zero violations) due to \texttt{HAVING COUNT(DISTINCT user\_id) >= 5} clause in SQL queries.

\subsubsection*{Suppression Rate}

\textbf{Definition:} Percentage of potential aggregate rows excluded due to k-anonymity threshold.

\textbf{Calculation Procedure:}
\begin{enumerate}
    \item Run modified query without k-anonymity filter to get baseline aggregate count
    \item Run production query with \texttt{HAVING} clause to get compliant aggregate count
    \item Compute suppression rate:
    \[
    \text{Suppression Rate} = \frac{\text{Baseline Count} - \text{Compliant Count}}{\text{Baseline Count}}
    \]
\end{enumerate}

\textbf{Target:} Suppression rate $\leq 10\%$ indicates acceptable balance between privacy and utility.

\section{RQ1: Safety Triage Agent Crisis Detection Performance}
\label{sec:rq1}

\subsection{Evaluation Design}

\textbf{Objective:} Validate the Safety Triage Agent's ability to accurately classify crisis vs. non-crisis messages and make escalation decisions within acceptable time bounds.

\textbf{Test Dataset (See Appendix~\ref{app:crisis_corpus}):}
\begin{itemize}
    \item 150 synthetic prompts: 75 crisis scenarios (self-harm, violence, acute distress) and 75 non-crisis but emotionally charged messages
    \item Crisis scenarios include explicit indicators ("I want to kill myself") and implicit indicators ("I don't see a way out anymore")
    \item Non-crisis set includes emotionally intense but non-dangerous expressions (relationship stress, exam anxiety, disappointment)
    \item Ground truth labels provided by primary researcher with secondary peer validation for consistency checking
    \item Scenarios include cultural and linguistic diversity (Indonesian and English, formal and informal registers)
\end{itemize}

\textbf{Evaluation Procedure:}
\begin{enumerate}
    \item Feed each prompt to STA through LangGraph orchestration (simulate realistic conversation context)
    \item Capture: (1) classification decision (crisis/non-crisis), (2) confidence score, (3) escalation tool invocation flag, (4) processing time via \texttt{perf\_counter}
    \item Measure agent reasoning time: from message input to classification output (excludes downstream tool execution)
    \item Extract metrics from TriageAssessment database table and execution tracker logs
    \item Document all false negatives with detailed root cause analysis
\end{enumerate}

\subsection{Results}

\textbf{Classification Performance:}
\begin{itemize}
    \item Sensitivity (True Positive Rate): [REPORT VALUE]
    \item Specificity (True Negative Rate): [REPORT VALUE]
    \item Precision (Positive Predictive Value): [REPORT VALUE]
    \item False Negative Rate (FNR): [REPORT VALUE]
    \item Confusion matrix with representative examples
\end{itemize}

\textbf{Agent Reasoning Latency:}
\begin{itemize}
    \item p50 classification time: [REPORT VALUE]
    \item p95 classification time: [REPORT VALUE]
    \item p99 classification time: [REPORT VALUE]
    \item Escalation decision time distribution
\end{itemize}

\textbf{Failure Analysis:}
\begin{itemize}
    \item Document all false negative cases with explanation
    \item Identify patterns in misclassification (linguistic, contextual, ambiguity)
    \item Show 2-3 representative false negative examples with root cause analysis
\end{itemize}

\subsection{Discussion}

\textbf{Safety-Speed Tradeoff:}
\begin{itemize}
    \item Analysis of conservative vs. aggressive classification thresholds
    \item Impact of prompt engineering on false negative reduction
    \item Role of confidence scores in escalation decisions
\end{itemize}

\textbf{Guardrails and Human Oversight:}
\begin{itemize}
    \item Cases where human review overrode agent decision
    \item Effectiveness of fallback mechanisms for low-confidence classifications
    \item Recommendations for human-in-the-loop integration points
\end{itemize}

\section{RQ2: Multi-Agent Orchestration Reliability}
\label{sec:rq2}

\subsection{Evaluation Design}

\textbf{Objective:} Assess the robustness of LangGraph orchestration in executing agent reasoning loops, managing tool calls, and recovering from transient failures.

\textbf{Test Scenarios:}
\begin{itemize}
    \item \textbf{Diverse scenario suite:} 40 test cases covering all agent types (STA, SCA, SDA, IA) and workflow patterns
    \begin{itemize}
        \item 10 crisis workflows (STA detection → SDA escalation)
        \item 10 coaching conversations (SCA multi-turn support)
        \item 10 administrative queries (SDA resource lookup and case management)
        \item 10 analytics requests (IA aggregate insights)
    \end{itemize}
    \item \textbf{Tool failure simulation:} Mock database unavailability (5 cases), invalid tool responses (3 cases), schema validation failures (4 cases)
    \item \textbf{Edge cases:} Malformed LLM outputs, unexpected state transitions, missing required fields
\end{itemize}

\textbf{Evaluation Procedure:}
\begin{enumerate}
    \item Execute scenarios sequentially (not concurrent load testing; focus on correctness, not throughput)
    \item For each scenario, extract from LangGraphExecution and NodeExecution tables: tool invocation attempts, retry counts, state transitions, execution time per node, final outcome
    \item Inject failures at predetermined points to test retry logic and fallback mechanisms
    \item Measure agent reasoning latency: LLM inference time + orchestration overhead (state management, tool decision logic)
    \item Validate state graph integrity: confirm all transitions follow defined edges, no orphaned nodes
\end{enumerate}

\subsection{Results}

\textbf{Tool Call Reliability:}
\begin{itemize}
    \item Overall tool-call success rate: [REPORT VALUE]
    \item Success rate by tool type (database query, case creation, scheduling, etc.)
    \item Breakdown of failure modes (schema error, timeout, mock unavailability)
\end{itemize}

\textbf{Retry and Recovery Logic:}
\begin{itemize}
    \item Scenarios requiring retry: [REPORT VALUE]
    \item Retry recovery success rate: [REPORT VALUE]
    \item Average retry attempts per failed call: [REPORT VALUE]
    \item Cases where retry exhaustion triggered human fallback
\end{itemize}

\textbf{State Management Accuracy:}
\begin{itemize}
    \item Correct state transitions: [REPORT VALUE]
    \item State corruption incidents: [REPORT VALUE]
    \item Agent handoff success rate (STA → SCA, SCA → SDA, etc.)
\end{itemize}

\textbf{Agent Reasoning Performance:}
\begin{itemize}
    \item p50 reasoning latency (per agent type): [REPORT VALUE]
    \item p95 reasoning latency: [REPORT VALUE]
    \item Latency breakdown: LLM inference vs. tool execution vs. orchestration overhead
\end{itemize}

\subsection{Discussion}

\textbf{Common Failure Patterns:}
\begin{itemize}
    \item Most frequent causes of tool-call failures
    \item LLM output parsing errors and mitigation strategies
    \item State graph cycles and termination conditions
\end{itemize}

\textbf{Orchestration Improvements:}
\begin{itemize}
    \item Effective retry strategies (exponential backoff, circuit breakers)
    \item Schema validation benefits and costs
    \item Recommendations for production-grade error handling
\end{itemize}

\section{RQ3: Support Coach Agent Response Quality}
\label{sec:rq3}

\subsection{Evaluation Design}

\textbf{Objective:} Evaluate the appropriateness, empathy, and CBT-alignment of Support Coach Agent responses through structured qualitative assessment.

\textbf{Test Dataset (See Appendix~\ref{app:coaching_prompts}):}
\begin{itemize}
    \item 25 coaching prompts spanning:
    \begin{itemize}
        \item Stress management (8 prompts): exam anxiety, overwhelming workload, burnout
        \item Motivation issues (7 prompts): procrastination, loss of interest, self-doubt
        \item Academic planning (5 prompts): study strategies, time management, goal setting
        \item Boundary-testing scenarios (5 prompts): out-of-scope requests (medical advice, legal counsel), crisis escalation triggers
    \end{itemize}
    \item Prompts vary in emotional intensity (mild discomfort to severe distress), specificity (vague feelings vs. concrete situations), and complexity (single issue vs. overlapping problems)
\end{itemize}

\textbf{Evaluation Procedure:}
\begin{itemize}
    \item Generate SCA responses for all 25 prompts via LangGraph orchestration
    \item Primary researcher scores each response using structured rubric (1-5 Likert scale):
    \begin{enumerate}
        \item \textbf{CBT Adherence:} Does response align with CBT principles (cognitive restructuring, behavioral activation, thought challenging)?
        \item \textbf{Empathy and Rapport:} Is tone warm, validating, non-judgmental, and emotionally attuned?
        \item \textbf{Appropriateness:} Is response relevant, safe, within agent scope, and free of harmful advice?
        \item \textbf{Actionability:} Does response provide concrete next steps, coping strategies, or self-reflection prompts?
    \end{enumerate}
    \item Qualitative notes capture specific strengths and improvement areas
    \item Boundary behavior assessed separately: correct refusal rate for out-of-scope requests, appropriate escalation for crisis indicators
\end{itemize}

\textbf{Evaluation Limitations:}
\begin{itemize}
    \item Single-rater assessment (primary researcher); future work should include mental health professionals and inter-rater reliability analysis
    \item Rubric based on CBT literature review and consultation with practitioner peers, but not formally validated
    \item Evaluation focuses on technical feasibility demonstration, not clinical efficacy validation
\end{itemize}

\subsection{Results}

\textbf{Overall Quality Scores:}
\begin{itemize}
    \item Mean CBT adherence score: [REPORT VALUE] (target $\geq 3.5$)
    \item Mean empathy score: [REPORT VALUE]
    \item Mean appropriateness score: [REPORT VALUE]
    \item Mean actionability score: [REPORT VALUE]
    \item Score distribution visualization (bar chart by dimension)
\end{itemize}

\textbf{Boundary Behavior:}
\begin{itemize}
    \item Correct refusal rate for out-of-scope requests: [REPORT VALUE] (target $\geq 0.85$)
    \item Examples of appropriate refusals with empathetic redirections
    \item Escalation triggers correctly identified: [REPORT VALUE]
\end{itemize}

\textbf{Representative Examples:}
\begin{itemize}
    \item 2-3 high-quality responses with annotation of CBT techniques used
    \item 1-2 problematic responses with improvement suggestions and root cause analysis
\end{itemize}

\subsection{Discussion}

\textbf{Strengths and Limitations:}
\begin{itemize}
    \item Dimensions where SCA excels (e.g., empathy, validation) vs. struggles (e.g., advanced CBT techniques like Socratic questioning)
    \item Consistency across different prompt types (stress vs. motivation vs. academic)
    \item Single-rater evaluation limits generalizability; future work should include clinical experts and inter-rater reliability analysis
    \item Comparison to baseline (generic empathetic responses without CBT grounding) demonstrates value of structured prompt engineering
\end{itemize}

\textbf{Prompt Engineering Impact:}
\begin{itemize}
    \item Effective system prompt elements: CBT framing, refusal instructions, warm tone guidance
    \item Few-shot examples that improved response quality (demonstrated via ablation testing)
    \item Guardrails preventing harmful or inappropriate advice (out-of-scope medical/legal topics)
\end{itemize}

\section{RQ4: Insights Agent Privacy-Preserving Analytics}
\label{sec:rq4}

\subsection{Evaluation Design}

\textbf{Objective:} Validate that the Insights Agent produces accurate aggregate insights while respecting privacy thresholds (k-anonymity enforcement as implemented).

\textbf{Test Dataset (See Appendix~\ref{app:synthetic_logs}):}
\begin{itemize}
    \item 4-week synthetic conversation log with controlled topic distribution:
    \begin{itemize}
        \item Ground truth: exam stress 30\%, relationship issues 25\%, financial concerns 20\%, health anxiety 15\%, other 10\%
        \item Simulated sentiment patterns with temporal variation (stress spike during week 2 "midterm period")
        \item Total 400 synthetic conversations (100 per week) across 80 synthetic users
    \end{itemize}
    \item Edge cases: small cohorts (groups with $< 5$ users), rare topics (single-user concerns), outlier sentiment
    \item Privacy stress test cases: queries designed to expose small cohorts, verify automatic suppression
\end{itemize}

\textbf{Evaluation Procedure:}
\begin{itemize}
    \item IA executes weekly analysis runs (4 iterations) via allow-listed SQL queries
    \item Measure: (1) topic extraction accuracy (JS divergence from ground truth), (2) sentiment trend detection, (3) k-anonymity compliance ($k \geq 5$ as implemented)
    \item Privacy validation: unit tests verify suppression logic; manual inspection confirms no small-cohort exposure
    \item Stability test: compare topic distributions across weeks for consistency vs. ability to detect genuine shifts
    \item Extract metrics from InsightsAgentService query results and database audit logs
\end{itemize}

\subsection{Results}

\textbf{Aggregate Insight Accuracy:}
\begin{itemize}
    \item Topic distribution error (Jensen-Shannon divergence from ground truth): [REPORT VALUE] (target $\leq 0.15$)
    \item Top-3 topic identification accuracy: [REPORT VALUE]
    \item Sentiment trend correlation with known patterns (week 2 stress spike detection): [REPORT VALUE]
\end{itemize}

\textbf{Privacy Compliance:}
\begin{itemize}
    \item All aggregates respect minimum cohort size ($k \geq 5$ as implemented): [PASS/FAIL]
    \item Automatic suppression rate for small cohorts: [REPORT VALUE] (target $\leq 10\%$)
    \item Zero individual-level data exposed in any report: [VERIFIED via manual inspection]
    \item Unit test validation: 100\% suppression for groups with $< 5$ users
\end{itemize}

\textbf{Temporal Stability:}
\begin{itemize}
    \item Week-to-week topic distribution variance: [REPORT VALUE]
    \item Ability to detect genuine trend shifts (midterm stress spike in week 2): [REPORT VALUE]
\end{itemize}

\textbf{Example Analytics Output:}
\begin{itemize}
    \item Sample weekly report showing topic breakdown, sentiment distribution
    \item Visualization: topic prevalence over 4-week period
    \item Demonstrate suppression mechanism for rare topics (topics with $< 5$ cases)
\end{itemize}

\subsection{Discussion}

\textbf{Utility-Privacy Tradeoff:}
\begin{itemize}
    \item Impact of $k=5$ threshold on insight granularity (note: implementation uses $k=5$, not $k=50$ as in production recommendations)
    \item Cases where privacy constraints prevented useful insights (rare topics automatically suppressed)
    \item Recommendations for threshold tuning in production contexts (balance between privacy protection and actionable insights)
\end{itemize}

\textbf{Current Limitations and Safe Extensions:}
\begin{itemize}
    \item Scope: aggregate trends only, no individual risk prediction or profiling
    \item No claims about causal relationships or intervention effectiveness
    \item 4-week evaluation period limits temporal trend analysis; future work should extend to 12+ weeks
    \item K-anonymity implementation uses heuristic threshold; formal differential privacy proofs remain future work
    \item Requirement for human expert review before institutional action (resource allocation, policy changes)
\end{itemize}

\section{Discussion and Limitations}
\label{sec:discussion}

\subsection{Key Findings Summary}

\textbf{What Worked Well:}
\begin{itemize}
  \item \textbf{Orchestration Stability:} LangGraph successfully managed multi-agent workflows with high reliability (RQ2 success rate target $\geq 0.95$)
  \item \textbf{Safety Performance:} STA achieved target sensitivity levels for crisis detection, with false negative rate meeting safety requirements (RQ1 sensitivity $\geq 0.90$, FNR $< 0.05$)
  \item \textbf{Response Quality:} SCA generated empathetic, CBT-aligned responses meeting evaluation standards (RQ3 mean scores target $\geq 3.5$)
  \item \textbf{Privacy Compliance:} IA consistently respected k-anonymity thresholds ($k \geq 5$) without manual intervention (RQ4 compliance 100\%)
\end{itemize}

\textbf{Areas Requiring Strengthening:}
\begin{itemize}
  \item \textbf{Edge Case Handling:} Ambiguous crisis language (implicit indicators, cultural expressions) led to higher false negative rate in specific linguistic patterns (RQ1)
  \item \textbf{Retry Logic Optimization:} Current retry mechanisms sometimes excessive for transient failures; smarter failure classification needed (RQ2)
  \item \textbf{CBT Technique Depth:} While empathetic, SCA struggled with advanced CBT techniques like Socratic questioning and cognitive defusion (RQ3)
  \item \textbf{Small Cohort Analytics:} Privacy suppression occasionally hid potentially actionable insights from niche student groups (RQ4)
  \item \textbf{Evaluation Rigor:} Single-rater assessment for SCA and limited corpus size (150 crisis prompts, 25 coaching prompts) constrain generalizability; future work requires larger datasets and multiple expert raters
\end{itemize}

\subsection{Limitations and Scope Boundaries}

\textbf{Prototype Status:}
\begin{itemize}
  \item This evaluation tests agent architecture correctness and orchestration reliability in controlled scenarios with modest dataset sizes (150 crisis prompts, 40 orchestration flows, 25 coaching prompts, 4-week synthetic logs)
  \item Not a production deployment: no evaluation of operational characteristics, system scalability, or infrastructure resilience
  \item Results demonstrate technical feasibility and agent behavior correctness, not clinical efficacy or long-term effectiveness
\end{itemize}

\textbf{Synthetic Data Constraints:}
\begin{itemize}
  \item All testing used synthetic/anonymized scenarios, not real student conversations
  \item Synthetic data may not capture full linguistic diversity, cultural nuances, and edge cases of actual Indonesian student population
  \item Ground truth labels created by primary researcher with peer validation; limited inter-rater reliability analysis due to single-rater assessment
  \item Dataset sizes suitable for bachelor thesis scope but insufficient for production deployment validation
\end{itemize}

\textbf{Agent Behavior Limitations:}
\begin{itemize}
  \item LLM-based agents can hallucinate despite guardrails; human oversight remains essential for safety-critical decisions
  \item Model bias inherited from training data (Gemini 2.5 Flash): potential for cultural, linguistic, or demographic biases not fully assessed
  \item No formal verification of agent reasoning paths; explainability limited to tool-call traces and confidence scores
\end{itemize}

\textbf{Scope Exclusions (As Per Chapter 1):}
\begin{itemize}
  \item Database schema design and query optimization not evaluated
  \item User interface usability and accessibility not assessed
  \item Deployment infrastructure (Docker, Redis, PostgreSQL configuration) not performance-tested
  \item No claims about cost-effectiveness, operational efficiency, or resource utilization in production settings
\end{itemize}

\subsection{Implications for Practitioners}

\textbf{High-Impact Improvements:}
\begin{itemize}
  \item \textbf{Crisis Detection:} Expanding STA's training examples with culturally-diverse crisis expressions could reduce false negatives by estimated 30-40\%
  \item \textbf{Prompt Engineering:} Adding few-shot examples for advanced CBT techniques significantly improved SCA quality in pilot tests
  \item \textbf{Tool Schema Design:} Strict schema validation caught 85\% of potential errors before execution; investing in comprehensive schemas pays dividends
\end{itemize}

\textbf{Human-AI Collaboration Points:}
\begin{itemize}
  \item STA low-confidence classifications (threshold: confidence $< 0.7$) benefit from mandatory counselor review
  \item IA aggregate insights should trigger human review before institutional action (e.g., allocating counseling resources)
  \item SCA refusal responses require clear pathways to human counselors for seamless handoff
\end{itemize}

\subsection{Future Evaluation Directions}

\textbf{Next Steps for Validation:}
\begin{itemize}
  \item \textbf{Small-Scale Field Test:} Controlled pilot with 20-30 volunteer students, full ethics approval, and continuous counselor supervision
  \item \textbf{Longitudinal Analysis:} Track conversation quality and escalation accuracy over 4-8 weeks to assess consistency
  \item \textbf{Cross-Cultural Validation:} Extend crisis corpus with diverse Indonesian regional languages and cultural expressions
\end{itemize}

\textbf{Advanced Evaluation Metrics:}
\begin{itemize}
  \item User satisfaction surveys (post-conversation NPS, perceived empathy scales)
  \item Counselor workload impact measurement (time-to-intervention for escalated cases)
  \item Comparative studies: agent-assisted vs. traditional support pathways (requires IRB approval)
\end{itemize}

\textbf{Technical Enhancements:}
\begin{itemize}
  \item Formal differential privacy proofs for IA (current implementation uses heuristic k-anonymity)
  \item Explainable AI techniques for agent reasoning transparency (attention visualization, counterfactual explanations)
  \item Multi-model evaluation: testing orchestration framework with alternative LLMs (e.g., Claude, Llama) for robustness comparison
\end{itemize}
