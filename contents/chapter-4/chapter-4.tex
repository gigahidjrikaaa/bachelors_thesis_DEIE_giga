\chapter{Implementation and Evaluation (Hasil dan Pembahasan)}

This chapter reports how the prototype was exercised and what we learned from it. The focus is on the agents and their behavior in safety‑relevant scenarios. We keep the scope practical and transparent so results can be reproduced and audited.

\section{Setup and Test Design (Rancangan Pengujian)}
\label{sec:setup}

This section documents the evaluation protocol that links the Design Science stages in Chapter~\ref{chap:system_design} to the research questions in Chapter~\ref{sec:research_questions}. Figure~\ref{fig:evaluation_pipeline} and Table~\ref{tab:evaluation_plan} provide a visual and tabular overview of the assets, metrics, and acceptance thresholds used throughout the chapter.

\subsection*{Evaluation Environment}

\begin{itemize}
    \item \textbf{Agents under test}: Safety Triage Agent (STA), Support Coach Agent (SCA), Service Desk Agent (SDA), and Insights Agent (IA) running inside the LangGraph orchestration described in Chapter~\ref{chap:system_design}. All tool invocations are captured through structured logs to enable replay and auditing.
    \item \textbf{Execution platform}: FastAPI backend deployed in a containerised environment (Python~3.11, Uvicorn workers $=8$) with Redis for task queues and PostgreSQL~15 for persistence. Tests are executed on a machine equivalent to 8~vCPU/32~GB RAM to mirror expected production sizing.
    \item \textbf{Instrumentation}: OpenTelemetry traces capture latency, tool-call success, and retries; custom middleware records human hand-offs, while differential privacy parameters are logged for the IA.
\end{itemize}

\subsection*{Datasets and Scenario Assets}

\begin{itemize}
    \item \textbf{Crisis corpus}: 500 labeled prompts covering self-harm, violence, and acute distress, augmented with 300 non-crisis but emotionally charged messages to measure false positives. Labels are derived from established mental health crisis assessment guidelines and validated through systematic comparison with published crisis intervention frameworks.
    \item \textbf{Coaching prompts}: 120 conversation snippets spanning stress management, motivation, academic planning, and administrative queries. Responses include canonical "out-of-scope" triggers to exercise refusal and escalation behaviour.
    \item \textbf{Operational events}: Synthetic scheduling and case-management payloads to drive SDA workflows, and a 12-week anonymised log (synthetic) to test IA stability and privacy thresholds (minimum cohort size $k=50$, placeholder $\epsilon=1.0$).
\end{itemize}

\subsection*{Quality Control and Validation}

\begin{itemize}
    \item \textbf{Safety validation}: All STA critical detections are validated against ground-truth labels derived from established crisis assessment frameworks. Disagreements between system output and expected classifications are logged and analyzed for patterns in Chapter~\ref{sec:discussion}.
    \item \textbf{Coaching quality rubric}: Three independent raters score SCA responses on CBT adherence, empathy, and appropriateness using a standardized 1--5 Likert scale rubric. Inter-rater reliability (Cohen's $\kappa$) is calculated and reported alongside mean scores to ensure assessment consistency.
    \item \textbf{Analytics verification}: IA outputs are inspected for k-anonymity compliance; any aggregate below the threshold is expected to be suppressed automatically.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \resizebox{0.85\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=2.6cm,
        stage/.style={rectangle, rounded corners=6pt, draw=ugmBlue, very thick, fill=ugmBlue!7, align=center, minimum width=3.5cm, minimum height=1.1cm},
        data/.style={rectangle, draw=ugmGold!80!black, thick, rounded corners=6pt, fill=ugmGold!25, align=center, minimum width=3.3cm, minimum height=1.1cm},
        arrow/.style={-Latex, thick, ugmBlue}
    ]
        \node[data] (datasets) {Scenario Assets\\Crisis corpus\\Coaching prompts\\Synthetic logs};
        \node[stage, right=of datasets] (agents) {Agents Under Test\\STA, SCA, SDA, IA\\LangGraph orchestration};
        \node[stage, right=of agents] (metrics) {Instrumentation\\Latency, accuracy, retries\\Privacy counters};
        \node[stage, right=of metrics] (oversight) {Quality Control\\Validation checks\\CBT raters};
        \node[stage, right=of oversight] (reporting) {Reporting\\RQ-specific dashboards\\Chapter~IV sections};

        \draw[arrow] (datasets) -- (agents);
        \draw[arrow] (agents) -- (metrics);
        \draw[arrow] (metrics) -- (oversight);
        \draw[arrow] (oversight) -- (reporting);
    \end{tikzpicture}}
    \caption{Evaluation workflow linking scenario assets, instrumentation, and quality control validation to the reporting structure in Chapter~IV.}
    \label{fig:evaluation_pipeline}
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Evaluation plan mapped to research questions and acceptance thresholds.}
    \label{tab:evaluation_plan}
    \small % Reduce font size to fit
    \setlength{\tabcolsep}{3pt} % Reduce column separation
    \begin{tabular}{p{1.6cm}p{2.8cm}p{3.0cm}p{2.8cm}p{2.0cm}}
        \toprule
        \textbf{RQ} & \textbf{Test Scenarios / Data} & \textbf{Primary Metrics} & \textbf{Success Criteria (Target)} & \textbf{Related Section} \\
        \midrule
        RQ1 (Safety) & 500 crisis/non-crisis prompts; end-to-end escalation drills & Sensitivity, specificity, precision, FNR, detection latency p95/p99 & Sensitivity $\geq 0.95$, FNR $< 0.02$, escalation $<30$~s, p95 latency $<0.25$~s & \S\ref{sec:rq1} \\
        RQ2 (Reliability) & Conversational stress test (500 concurrent sessions); failure injection (API outage, malformed payload) & Tool-call success rate, mean retries per call, workflow completion \%, latency p50/p95 & Success rate $\geq 0.98$, average retries $\leq 0.3$, workflow completion $\geq 0.97$, p95 latency $<1.5$~s & \S\ref{sec:rq2} \\
        RQ3 (Quality) & 120 coaching prompts scored by 3 raters & \multicolumn{1}{p{3.0cm}}{CBT adherence score, empathy, refusal accuracy, inter-rater $\kappa$} & Mean scores $\geq 4$ (out of 5), $\kappa \geq 0.75$, correct refusal $\geq 0.9$ & \S\ref{sec:rq3} \\
        RQ4 (Insights) & 12-week synthetic log with known topic distribution; privacy stress test & Topic stability (Jensen-Shannon divergence), sentiment drift, suppression rate, DP noise magnitude & Divergence $\leq 0.1$, all aggregates respect $k\geq 50$, suppression rate $\leq 5\%$, DP noise within planned bounds & \S\ref{sec:rq4} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{RQ1 - Safety: Can STA detect crises promptly?}
\label{sec:rq1}

\textbf{Desain}. Uji pada set krisis sintetis dengan label. Ukur sensitivitas, spesifisitas, dan waktu ke eskalasi dari deteksi awal hingga pembuatan tiket/alert. Analisis khusus pada kegagalan berisiko (\textit{false negatives}).

\textbf{Hasil}. Ringkas angka utama (mis. sensitivitas, spesifisitas, p50/p95 latensi). Tampilkan contoh sukses dan kegagalan yang representatif.

\textbf{Bahasan}. Kompromi antara kecepatan dan kehati‑hatian; peran \textit{guardrail} dan \textit{fallback} manusia.

\section{RQ2 — Reliability: Does orchestration run reliably?}
\label{sec:rq2}

\textbf{Desain}. Telusuri rasio keberhasilan pemanggilan fungsi, validasi skema, \textit{retry/backoff}, dan penyelesaian \textit{workflow} ujung‑ke‑ujung.

\textbf{Hasil}. Laporkan tingkat keberhasilan, tingkat kegagalan yang pulih, dan kasus terhenti (jika ada). Sertakan latensi p50/p95 per langkah.

\textbf{Bahasan}. Pola kegagalan yang paling sering dan perbaikan yang mudah diterapkan.

\section{RQ3 — Quality: Are SCA responses reasonable and CBT‑informed?}
\label{sec:rq3}

\textbf{Desain}. Penilaian buta oleh evaluator pada sampel percakapan (kecil namun beragam). Rubrik menilai kepatuhan CBT dasar, keamanan saran, dan empati.

\textbf{Hasil}. Skor ringkas dan contoh tanggapan baik/kurang baik. Catat penolakan yang tepat pada topik di luar batas.

\textbf{Bahasan}. Pola perbaikan prompt/alat yang berdampak nyata.

\section{RQ4 — Insights (minimal): Can IA produce safe aggregate views?}
\label{sec:rq4}

\textbf{Desain}. Jalur agregasi sederhana: ambang privasi (mis. k‑anonymity) dan cek kestabilan jumlah/topik. Tidak ada klaim level individu.

\textbf{Hasil}. Laporkan hanya \textit{sanity check} agregat (mis. stabil/variatif) dan kepatuhan terhadap ambang privasi.

\textbf{Bahasan}. Keterbatasan desain saat ini dan langkah aman untuk perluasan.

\section{Discussion and Limitations (Diskusi dan Keterbatasan)}
\label{sec:discussion}

\begin{itemize}
  \item \textbf{Temuan utama}. Soroti apa yang berjalan baik (mis. orkestrasi stabil, latensi terkendali) dan apa yang perlu diperkuat (mis. penanganan tepi kasus tertentu).
  \item \textbf{Batasan}. Prototipe, data sintetis/anonym, tidak ada klaim efek klinis; model dapat bias/\textit{hallucinate} meski ada \textit{guardrail}.
  \item \textbf{Implikasi}. Perbaikan sederhana yang memberi dampak besar; rencana evaluasi lanjutan (\textit{field} kecil) dengan pengawasan etik yang memadai.
\end{itemize}
